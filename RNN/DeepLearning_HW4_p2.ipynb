{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"authorship_tag":"ABX9TyP0CINCFn69a6yshcTfxPZT","collapsed_sections":["w_a3OXnSeV0z","RwaY_YcgRayy","RTql4Ftiunfr","ujIVtjsYvxOI","wCi-ofSLCzop","B29jrEvwRqXA","A3rnR739GbYb","a4HyLPqcsF43","cfBasjQCE_aI","idRexFij4wgN","PgLgP04P4-aX","NCQjacybOfqV","3ttl0AK3Hvyh","24qT-sgUO2-d","W0QNbC0YPCKZ","G9HgVWslPGsH","o_5f69nwPtY2","De7VreNxQdct","lpJ3wtyctQJH","BrHQCv7q7LF_","BLT4w0ZfAhlJ","uC2GhaXfA8vC","Mjd9Z3N1ef3I","rjGQ-M02cusP","oK20iNRI3Xxb","KZ9UIdmkfxlA","FzcQQwFuar_7"],"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<br>\n<font>\n<div dir=ltr align=center>\n<font color=0F5298 size=10>\nÂ Â Â  Deep Learning - HW4 <br>\n<font color=2565AE size=5>\nÂ Â Â  Electrical Engineering Department <br>\nÂ Â Â  winter 2024<br>\n<font color=3C99D size=5>\n    Practical Assignment 2 <br>\n<font color=696880 size=4>\nÂ Â Â  Armin Ghojehzadeh \n\n____","metadata":{"id":"w_a3OXnSeV0z"}},{"cell_type":"markdown","source":"# ğŸ”´ **Import Libs**","metadata":{}},{"cell_type":"code","source":"!pip uninstall torchtext -y\n!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install --upgrade torchdata\n!pip install --upgrade torchtext --index-url https://download.pytorch.org/whl/cu118\n!pip install torchmetrics","metadata":{"execution":{"iopub.status.busy":"2024-12-28T17:04:54.038205Z","iopub.execute_input":"2024-12-28T17:04:54.038506Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.5.1%2Bcu118-cp310-cp310-linux_x86_64.whl (838.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m838.3/838.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.1%2Bcu118-cp310-cp310-linux_x86_64.whl (6.5 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.1%2Bcu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.1.0 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting sympy==1.13.1 (from torch)\n  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\nInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.3\n    Uninstalling sympy-1.13.3:\n      Successfully uninstalled sympy-1.13.3\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.1+cu121\n    Uninstalling torch-2.4.1+cu121:\n      Successfully uninstalled torch-2.4.1+cu121\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.19.1+cu121\n    Uninstalling torchvision-0.19.1+cu121:\n      Successfully uninstalled torchvision-0.19.1+cu121\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.4.1+cu121\n    Uninstalling torchaudio-2.4.1+cu121:\n      Successfully uninstalled torchaudio-2.4.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.17 requires torch<2.5,>=1.10, but you have torch 2.5.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.1 torch-2.5.1+cu118 torchaudio-2.5.1+cu118 torchvision-0.20.1+cu118 triton-3.1.0\nCollecting torchdata\n  Downloading torchdata-0.10.1-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.2.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.32.3)\nRequirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.5.1+cu118)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.8.86)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2->torchdata) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\nDownloading torchdata-0.10.1-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport torchtext\nimport torchdata\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom torch import optim\nfrom torch.nn import functional as F\n\nimport tqdm\nimport torchmetrics as tm","metadata":{"id":"vhlVJEkJeTsV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python --version\nprint(torch.__version__)\nprint(torchtext.__version__)\nprint(torchdata.__version__)","metadata":{"id":"DEzYlyeqTZqQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for lib in [np, torch, torchtext, tqdm]:\n  print(lib.__name__, '-->', lib.__version__)","metadata":{"id":"6DWjGTq6T8Jg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ğŸ”´ **Utils**","metadata":{"id":"RwaY_YcgRayy"}},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"id":"8yMS7bbmRayz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def num_trainable_params(model):\n  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n  return nums","metadata":{"id":"PpKbTUEIRayz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ğŸ”´ **Dataset**","metadata":{"id":"RTql4Ftiunfr"}},{"cell_type":"markdown","source":"## ğŸŸ  **Load the Dataset**","metadata":{"id":"ujIVtjsYvxOI"}},{"cell_type":"markdown","source":"ğŸ”° In this session you should load WikiText2 dataset.","metadata":{"id":"Ek9DpCNCChzF"}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n\ntrain_data = dataset['train']\nval_data = dataset['validation']\ntest_data = dataset['test']\nprint(train_data[1]) ","metadata":{"id":"ShYpXvVzVmP6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸŸ  **Build vocabulary and save it**","metadata":{"id":"wCi-ofSLCzop"}},{"cell_type":"markdown","source":"ğŸ”° In this section we need to:\n\n*   Define a tokenizer using `basic_english`\n*   Tokenize the dataset and collect tokens\n*   Build the vocabulary using `build_vocab_from_iterator`\n*   Manually insert special tokens and set the default index\n","metadata":{"id":"L02PHFuyNRb3"}},{"cell_type":"code","source":"from torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer(\"basic_english\")\n\ndef yield_tokens(dataset_split):\n    for example in dataset_split:\n        yield tokenizer(example[\"text\"])\n\ntrain_tokens = yield_tokens(train_data)\n\nvocab = build_vocab_from_iterator(train_tokens, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\n\nprint(\"Special Tokens and Their Indices:\")\nprint(f\"<unk>: {vocab['<unk>']}\")\nprint(f\"<pad>: {vocab['<pad>']}\")\nprint(f\"<bos>: {vocab['<bos>']}\")\nprint(f\"<eos>: {vocab['<eos>']}\")\n\n\nsample_text = \"This is an example sentence.\"\ntokenized_text = tokenizer(sample_text)\nindexed_text = [vocab[token] for token in tokenized_text]\n\nprint(\"\\nTokenized Text:\", tokenized_text)\nprint(\"Indexed Text:\", indexed_text)","metadata":{"id":"dlJ6Q6xCVuf0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸŸ  EDA","metadata":{"id":"B29jrEvwRqXA"}},{"cell_type":"markdown","source":"### ğŸŸ¡ Let's explore the WikiText2 dataset!","metadata":{"id":"pHtoYxEPd3bL"}},{"cell_type":"markdown","source":"### ğŸŸ¡ Calculate basic statistics such as the number of documents, total words, average document length, etc.","metadata":{"id":"A3rnR739GbYb"}},{"cell_type":"code","source":"def calculate_statistics(dataset_split):\n    num_documents = len(dataset_split)\n    total_words = 0\n    total_lengths = []\n\n    for doc in dataset_split:\n        tokens = tokenizer(doc[\"text\"])\n        total_words += len(tokens)\n        total_lengths.append(len(tokens))\n\n    avg_doc_length = total_words / num_documents if num_documents > 0 else 0\n\n    return {\n        \"num_documents\": num_documents,\n        \"total_words\": total_words,\n        \"avg_doc_length\": avg_doc_length,\n        \"min_doc_length\": min(total_lengths) if total_lengths else 0,\n        \"max_doc_length\": max(total_lengths) if total_lengths else 0,\n    }\n\ntrain_stats = calculate_statistics(train_data)\nprint(\"Training Set Statistics:\")\nfor stat, value in train_stats.items():\n    print(f\"{stat}: {value}\")","metadata":{"id":"jHVKeKwk2WaG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸŸ¡ Analyze the most common and least common words in the dataset.","metadata":{"id":"a4HyLPqcsF43"}},{"cell_type":"code","source":"from collections import Counter\n\ndef count_words(dataset_split):\n    word_counter = Counter()\n\n    for doc in dataset_split:\n        tokens = tokenizer(doc[\"text\"])\n        word_counter.update(tokens)\n\n    return word_counter\n\ntrain_word_counts = count_words(train_data)\nmost_common_words = train_word_counts.most_common(10)\nleast_common_words = [word for word, count in train_word_counts.items() if count == 1]\n\nprint(\"Most Common Words:\")\nfor word, count in most_common_words:\n    print(f\"{word}: {count}\")\n\nprint(\"\\nNumber of Least Common Words (occurring once):\", len(least_common_words))\nprint(\"Example of Least Common Words:\", least_common_words[:10])\n","metadata":{"id":"cBnEjagdTN8n","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸŸ¡  Please proceed with further exploration of the dataset. what do you suggest?","metadata":{"id":"cfBasjQCE_aI"}},{"cell_type":"code","source":"word_frequencies = np.array(list(train_word_counts.values()))\n\nplt.figure(figsize=(10, 6))\nplt.hist(word_frequencies, bins=50, log=True, color='blue', alpha=0.7)\nplt.title(\"Word Frequency Distribution (Log Scale)\")\nplt.xlabel(\"Frequency\")\nplt.ylabel(\"Number of Words\")\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\nsorted_frequencies = np.sort(word_frequencies)[::-1]\nranks = np.arange(1, len(sorted_frequencies) + 1)\n\nplt.figure(figsize=(10, 6))\nplt.loglog(ranks, sorted_frequencies, marker=\"o\", linestyle=\"\", markersize=3, color='red')\nplt.title(\"Word Frequencies (Log-Log Plot)\")\nplt.xlabel(\"Rank\")\nplt.ylabel(\"Frequency\")\nplt.grid(which='both', linestyle='--', alpha=0.7)\nplt.show()","metadata":{"id":"yR8uQsv4E_aJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸŸ  Transform the data","metadata":{"id":"idRexFij4wgN"}},{"cell_type":"markdown","source":"ğŸ›‘ Make sure to perform the transformations on train, validation and test datasets.","metadata":{"id":"2VjvBOtvHu2v"}},{"cell_type":"markdown","source":"ğŸ”° Reshape the dataset into an `N x B x L` or `M x L` format, where `N` represents the number of batches, `B` is the batch size, `L` is the length of a sample within each batch, and `M` is equal to `N x B`.","metadata":{"id":"ApisIcGeGSsJ"}},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef data_process(raw_text_iter, batch_size, seq_len, vocab, tokenizer):\n    tokens = []\n    for line in raw_text_iter:\n        tokens.extend(tokenizer(line))\n    \n    token_indices = torch.tensor([vocab[token] for token in tokens if token in vocab])\n    \n    total_tokens = len(token_indices)\n    num_samples = total_tokens // seq_len\n    token_indices = token_indices[:num_samples * seq_len]\n    data = token_indices.clone().detach().view(num_samples, seq_len)\n\n    inputs = data[:, :-1]\n    targets = data[:, 1:]\n    return inputs, targets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\nseq_len = 50\n\ntrain_raw_iter = (line[\"text\"] for line in train_data)\nval_raw_iter = (line[\"text\"] for line in val_data)\ntest_raw_iter = (line[\"text\"] for line in test_data)\n\ntrain_inputs, train_targets = data_process(train_raw_iter, batch_size, seq_len, vocab, tokenizer)\nval_inputs, val_targets = data_process(val_raw_iter, batch_size, seq_len, vocab, tokenizer)\ntest_inputs, test_targets = data_process(test_raw_iter, batch_size, seq_len, vocab, tokenizer)\n\nprint(\"Inputs shape (N x B x L):\", train_inputs.shape)\nprint(\"Targets shape (N x B x L):\", train_targets.shape)\n","metadata":{"id":"4GndG2B0WPIb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸŸ  Custom dataset","metadata":{"id":"PgLgP04P4-aX"}},{"cell_type":"markdown","source":"ğŸ”° Write a custom dataset class for LanguageModelDataset.","metadata":{"id":"XkxH_IR2PBNq"}},{"cell_type":"code","source":"class LanguageModelDataset(Dataset):\n\n  def __init__(self, inputs, targets):\n    self.inputs = inputs\n    self.targets = targets\n\n  def __len__(self):\n    return self.inputs.shape[0]\n\n  def __getitem__(self, idx):\n    return self.inputs[idx], self.targets[idx]\n","metadata":{"id":"1cjpSkrtexap","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = LanguageModelDataset(train_inputs, train_targets)\nval_dataset = LanguageModelDataset(val_inputs, val_targets)\ntest_dataset = LanguageModelDataset(test_inputs, test_targets)\n\nprint(f\"Dataset length: {len(train_dataset)}\")\nprint(\"Sample input-target pair:\")\nprint(train_dataset[0])","metadata":{"id":"o0qUkL0CfQmr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸŸ  Define a dataloader if needed","metadata":{"id":"NCQjacybOfqV"}},{"cell_type":"markdown","source":"ğŸ”° Write dataloaders for the training, validation, and test sets.","metadata":{"id":"HqKMEyFNS-1a"}},{"cell_type":"code","source":"batch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)","metadata":{"id":"KMCJ3UMD0U_f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ğŸ”´ **Model**","metadata":{"id":"3ttl0AK3Hvyh"}},{"cell_type":"markdown","source":"ğŸ”° Use the following template to create a custom model.\n\nYour model should consist of three parts:\n\n*   an embedding layer\n*   an LSTM layer\n*   a fully connected layer","metadata":{"id":"06p-oBowTf-R"}},{"cell_type":"code","source":"class LanguageModel(nn.Module):\n  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate):\n    super(LanguageModel, self).__init__()\n      \n    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n    self.lstm = nn.LSTM(\n        input_size=embedding_dim,\n        hidden_size=hidden_dim,\n        num_layers=num_layers,\n        dropout=dropout_rate,\n        batch_first=True\n    )\n    self.fc = nn.Linear(hidden_dim, vocab_size)\n    self.dropout = nn.Dropout(dropout_rate)\n    \n\n  def forward(self, src):\n    embedded = self.embedding(src)\n    lstm_out, _ = self.lstm(embedded)\n    lstm_out = self.dropout(lstm_out)\n    output = self.fc(lstm_out)\n    return output","metadata":{"id":"ISnnHE0BMVqp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size = len(vocab)\nembedding_dim = 128\nhidden_dim = 256\nnum_layers = 2\ndropout_rate = 0.5\n\nmodel = LanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate)\nprint(model)\n\nsrc, _ = next(iter(train_loader))\noutput = model(src)\nprint(\"Output shape:\", output.shape)","metadata":{"id":"2MgBVzorb9oQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ğŸ”´ **Config**","metadata":{"id":"24qT-sgUO2-d"}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"id":"Ma28M5Z36gsq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ğŸ”° Define the optimizer, loss function, metrics and other necessary parameters in this section, and ensure the model is sent to the appropriate device.","metadata":{"id":"bwYDJKjuduUT"}},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\naccuracy = tm.Accuracy(task=\"multiclass\", num_classes=vocab_size).to(device)\nperplexity = tm.text.Perplexity().to(device)\n\nprint(\"Model Summary:\")\nprint(model)\nprint(f\"Optimizer: {optimizer}\")\nprint(f\"Loss Function: {criterion}\")\nprint(f\"Metric: {perplexity}\")","metadata":{"id":"9ubk3xKaIG6i","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ğŸ”´ **Train â°**","metadata":{"id":"W0QNbC0YPCKZ"}},{"cell_type":"markdown","source":"ğŸ”° This is the template for train function, change it if needed.","metadata":{"id":"yS6EF4HUhi5e"}},{"cell_type":"code","source":"scaler = torch.cuda.amp.GradScaler()\n\ndef train_one_epoch(model, train_loader, loss_fn, optimizer, metric, epoch=None):\n  model.train()\n  loss_train = AverageMeter()\n  metric.reset()\n\n  with tqdm.tqdm(train_loader, unit='batch') as tepoch:\n    for inputs, targets in tepoch:\n      if epoch:\n        tepoch.set_description(f'Epoch {epoch}')\n\n      inputs = inputs.to(device)\n      targets = targets.to(device)\n\n      outputs = model(inputs)\n        \n      outputs_2d = outputs.view(-1, outputs.size(-1))  \n      targets_2d = targets.view(-1)\n\n      loss = loss_fn(outputs_2d, targets_2d)\n\n      loss.backward()\n\n      optimizer.step()\n      optimizer.zero_grad()\n\n      loss_train.update(loss.item(), n=len(targets))\n        \n      logit = F.log_softmax(outputs, dim=-1)\n      metric.update(logit, targets)\n\n      tepoch.set_postfix(loss=loss_train.avg, metric=metric.compute().item())\n\n  return model, loss_train.avg, metric.compute().item()","metadata":{"id":"WniOAgk0QyRI","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ğŸ”´ **Evaluation**","metadata":{"id":"G9HgVWslPGsH"}},{"cell_type":"markdown","source":"ğŸ”° This is the template for evaluation function, change it if needed.","metadata":{"id":"TsszJ7GVj2l3"}},{"cell_type":"code","source":"def evaluate(model, test_loader, loss_fn, metric):\n  model.eval()\n  loss_eval = AverageMeter()\n  metric.reset()\n\n  with torch.inference_mode():\n    for inputs, targets in test_loader:\n      inputs = inputs.to(device)\n      targets = targets.to(device)\n\n      outputs = model(inputs)\n        \n      outputs_2d = outputs.view(-1, outputs.size(-1))  \n      targets_2d = targets.view(-1) \n\n      loss = loss_fn(outputs_2d, targets_2d)\n      loss_eval.update(loss.item(), n=len(targets))\n\n      logit = F.log_softmax(outputs, dim=-1)\n      metric.update(logit, targets)\n\n  return loss_eval.avg, metric.compute().item()","metadata":{"id":"uV0_67_ZQ0xf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ğŸ”´ **Training Process ã€½ï¸**","metadata":{"id":"o_5f69nwPtY2"}},{"cell_type":"markdown","source":"## ğŸŸ  Finding Hyper-parameters","metadata":{"id":"De7VreNxQdct"}},{"cell_type":"markdown","source":"### ğŸŸ¡ **Step 1:** Calculate the loss for an untrained model using a few batches.\n","metadata":{"id":"lpJ3wtyctQJH"}},{"cell_type":"code","source":"model = model.to(device)\nmodel.eval()\n\ninputs, targets = next(iter(train_loader))\ninputs = inputs.to(device)\ntargets = targets.to(device)\n\nwith torch.no_grad():\n  outputs = model(inputs)\n  \n  outputs_2d = outputs.view(-1, outputs.size(-1))  \n  targets_2d = targets.view(-1) \n    \n  print(outputs.shape, targets.shape)\n    \n  loss = criterion(outputs_2d, targets_2d)\n\nprint(loss)","metadata":{"id":"QnE4F4GkzzaR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸŸ¡ **Step 2:** Try to train and overfit the model on a small subset of the dataset.","metadata":{"id":"BrHQCv7q7LF_"}},{"cell_type":"code","source":"model = model.to(device)","metadata":{"id":"G0ji0MXsWaPt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_subset = torch.utils.data.Subset(train_dataset, indices=range(256))\ntrain_subset_loader = DataLoader(train_subset, batch_size=32, shuffle=True)","metadata":{"id":"kPRZQpPWJ2qv","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 30\n\nfor epoch in range(num_epochs):\n  model, train_loss, train_perplexity = train_one_epoch(model, train_subset_loader, criterion, optimizer, perplexity, epoch)\n  val_loss, val_perplexity = evaluate(model, val_loader, criterion, perplexity)\n\n  print(f\"Epoch {epoch} -- Train Loss: {train_loss:.4f} -- Train Perplexity: {train_perplexity:.4f} -- Val Loss: {val_loss:.4f} -- Val Perplexity: {val_perplexity:.4f}\")","metadata":{"id":"bNrg4d9hWaPt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸŸ¡ **Step 3:** Train the model for a limited number of epochs, experimenting with various learning rates.","metadata":{"id":"BLT4w0ZfAhlJ"}},{"cell_type":"code","source":"num_epochs = 10\n\nfor lr in [1e-2, 1e-3, 1e-4]:\n  print(f'LR={lr}')\n\n  model = LanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate).to(device)\n  optimizer = optim.SGD(model.parameters(), lr=lr)\n  criterion = nn.CrossEntropyLoss()\n  perplexity = tm.text.Perplexity().to(device)\n\n  for epoch in range(num_epochs):\n    model, train_loss, train_perplexity = train_one_epoch(model, train_loader, criterion, optimizer, perplexity, epoch)\n    val_loss, val_perplexity = evaluate(model, val_loader, criterion, perplexity)\n\n    print(f\"Epoch {epoch} -- Train Loss: {train_loss:.4f} -- Train Perplexity: {train_perplexity:.4f} -- Val Loss: {val_loss:.4f} -- Val Perplexity: {val_perplexity:.4f}\")\n\n  print(f\"Val Perplexity: {val_perplexity:.4f}\")","metadata":{"id":"Jxz5DXoj61mg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸŸ¡ Step 4: Create a small grid using the weight decay and the best learning rate.\n\n\n\n","metadata":{"id":"uC2GhaXfA8vC"}},{"cell_type":"code","source":"num_epochs = 10\n\nfor lr in [1e-2, 1e-3, 1e-4]:\n  for wd in [1e-2, 1e-3, 1e-4]:\n    print(f'LR={lr}, WD={wd}')\n\n    model = LanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate)\n    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd)\n\n    for epoch in range(num_epochs):\n      model, train_loss, train_perplexity = train_one_epoch(model, train_subset_loader, criterion, optimizer, perplexity, epoch)\n      val_loss, val_perplexity = evaluate(model, val_loader, criterion, accuracy)\n\n      print(f\"Epoch {epoch} -- Train Loss: {train_loss:.4f} -- Train Perplexity: {train_perplexity:.4f} -- Val Loss: {val_loss:.4f} -- Val Perplexity: {val_perplexity:.4f}\")\n\n  print(f\"Val Perplexity: {val_perplexity:.4f}\")","metadata":{"id":"a7UeNW3WWaPu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸŸ¡ Step 5: Train model for longer epochs using the best model from step 4.\n\n\n\n","metadata":{"id":"Mjd9Z3N1ef3I"}},{"cell_type":"code","source":"model = LanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate).to(device)","metadata":{"id":"IWgkMgC6JWpU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lr = 1e-3\nwd = 1e-3\noptimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9, nesterov=True)","metadata":{"id":"YVwLp-02JWpV","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_train_hist = []\nloss_valid_hist = []\n\nmetric_train_hist = []\nmetric_valid_hist = []\n\nbest_loss_valid = torch.inf\nepoch_counter = 0","metadata":{"id":"zqxSVVB7JWpW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 10\n\nfor epoch in range(num_epochs):\n  # Train\n  model, loss_train, metric_train = train_one_epoch(model,\n                                                 train_loader,\n                                                 criterion,\n                                                 optimizer,\n                                                 perplexity,\n                                                 epoch)\n  # Validation\n  loss_valid, metric_valid = evaluate(model,\n                                     val_loader,\n                                     criterion,\n                                     perplexity)\n\n  loss_train_hist.append(loss_train)\n  loss_valid_hist.append(loss_valid)\n\n  metric_train_hist.append(metric_train)\n  metric_valid_hist.append(metric_valid)\n\n  if loss_valid < best_loss_valid:\n    torch.save(model, f'model.pt')\n    best_loss_valid = loss_valid\n    print('Model Saved!')\n\n  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n  print()\n\n  epoch_counter += 1","metadata":{"id":"eVqS9SEPJWpW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸŸ  Main Loop","metadata":{"id":"rjGQ-M02cusP"}},{"cell_type":"markdown","source":"ğŸ”° Define model.","metadata":{"id":"4AdYaMU4x34g"}},{"cell_type":"code","source":"model = torch.load('model.pt').to(device)","metadata":{"id":"JCtZXDybxexf"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ğŸ”° Define optimizer and Set learning rate and weight decay.","metadata":{"id":"AUKZRiQPxqrB"}},{"cell_type":"code","source":"lr = 1e-3\nwd = 1e-3\noptimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9, nesterov=True)","metadata":{"id":"bowjVB5yIXUP"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ğŸ”° Write code to train the model for `num_epochs` epoches.","metadata":{"id":"AUyFFIzlyaiB"}},{"cell_type":"code","source":"loss_train_hist = []\nloss_valid_hist = []\n\nmetric_train_hist = []\nmetric_valid_hist = []\n\nbest_loss_valid = torch.inf\nepoch_counter = 0","metadata":{"id":"CAXagB4yvtZd"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 10\n\nfor epoch in range(num_epochs):\n  # Train\n  model, loss_train, metric_train = train_one_epoch(model,\n                                                 train_loader,\n                                                 criterion,\n                                                 optimizer,\n                                                 perplexity,\n                                                 epoch)\n  # Validation\n  loss_valid, metric_valid = evaluate(model,\n                                     val_loader,\n                                     criterion,\n                                     perplexity)\n\n  loss_train_hist.append(loss_train)\n  loss_valid_hist.append(loss_valid)\n\n  metric_train_hist.append(metric_train)\n  metric_valid_hist.append(metric_valid)\n\n  if loss_valid < best_loss_valid:\n    torch.save(model, f'model.pt')\n    best_loss_valid = loss_valid\n    print('Model Saved!')\n\n  print(f'Valid: Loss = {loss_valid:.4}, Metric = {metric_valid:.4}')\n  print()\n\n  epoch_counter += 1","metadata":{"id":"PovABWnU3ld0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸŸ  Plot","metadata":{"id":"oK20iNRI3Xxb"}},{"cell_type":"markdown","source":"ğŸ”° Plot learning curves","metadata":{"id":"IKlLvCwuzEAA"}},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\n\nplt.plot(range(epoch_counter), loss_train_hist, 'r-', label='Train')\nplt.plot(range(epoch_counter), loss_valid_hist, 'b-', label='Validation')\n\nplt.xlabel('Epoch')\nplt.ylabel('loss')\nplt.grid(True)\nplt.legend()","metadata":{"id":"KYFzTsdIOkVp"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ğŸ”´ **Test**","metadata":{"id":"KZ9UIdmkfxlA"}},{"cell_type":"markdown","source":"ğŸ”° Test your model using data from the test set","metadata":{"id":"SO8iPWH1zVYn"}},{"cell_type":"code","source":"test_loss, test_metric = evaluate(model, test_loader, criterion, perplexity)\nprint(f'Test: Loss = {test_loss:.4}, Metric = {test_metric:.4}')","metadata":{"id":"35sn67IhKcm_"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ğŸ”´ **Generate**","metadata":{"id":"FzcQQwFuar_7"}},{"cell_type":"markdown","source":"ğŸ”° Your mission is to write a `generate` function and use a desired sentence to evaluate the model","metadata":{"id":"jh2_9jUp0GF4"}},{"cell_type":"code","source":"model_path = 'model.pt'\nmodel = torch.load(model_path)\nmodel.eval()","metadata":{"id":"pskvb--R-wJ0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, seed=None):\n\n    if seed is not None:\n        torch.manual_seed(seed)\n    \n    model.eval()\n    \n    with torch.no_grad():\n        tokens = tokenizer(prompt)\n        token_indices = [vocab[token] for token in tokens if token in vocab]\n        token_tensor = torch.tensor(token_indices).unsqueeze(0)\n    \n        for _ in range(max_seq_len):\n            output = model(token_tensor)\n            output_probs = F.softmax(output[:, -1, :] / temperature, dim=-1)\n            token_probs, token_indices = output_probs.topk(1)\n    \n            token_index = token_indices.item()\n            token_tensor = torch.cat([token_tensor, torch.tensor([[token_index]])], dim=-1)\n        \n            if token_index == vocab['<eos>']:\n                break\n        \n    generated_tokens = [vocab.get_itos()[idx] for idx in token_tensor.squeeze(0).numpy()]\n    generated_text = tokenizer.convert_tokens_to_string(generated_tokens)\n    \n    return generated_text","metadata":{"id":"f5SvSDLal8YB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, seed=None):\n    if seed is not None:\n        torch.manual_seed(seed)\n\n    model.eval()\n    \n    # Tokenize the prompt and convert to tensor\n    tokens = tokenizer(prompt)  # e.g., [\"This\", \"is\", \"a\"]\n    token_indices = torch.tensor([vocab[token] for token in tokens], dtype=torch.long).unsqueeze(0).to(next(model.parameters()).device)\n    \n    generated = token_indices.clone()  # Start with the prompt\n    \n    with torch.no_grad():\n        for _ in range(max_seq_len):\n            logits = model(generated)\n            \n            logits = logits[:, -1, :]  # Shape: [1, vocab_size]\n            \n            logits = logits / temperature\n            \n            probabilities = F.softmax(logits, dim=-1)\n            \n            next_token_index = torch.multinomial(probabilities, num_samples=1).item()\n            \n            generated = torch.cat([generated, torch.tensor([[next_token_index]], device=generated.device)], dim=1)\n            \n            if next_token_index == vocab[\"<eos>\"]:\n                break\n\n    generated_text = \" \".join([vocab.lookup_token(idx) for idx in generated.squeeze().tolist()])\n    return generated_text","metadata":{"id":"pVedneOVD6ul"},"outputs":[],"execution_count":null}]}