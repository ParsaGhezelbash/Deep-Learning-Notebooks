{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=10>\n",
    "    Deep Learning - HW4 <br>\n",
    "<font color=2565AE size=5>\n",
    "    Electrical Engineering Department <br>\n",
    "    winter 2024<br>\n",
    "<font color=3C99D size=5>\n",
    "    Practical Assignment 3 <br>\n",
    "<font color=696880 size=4>\n",
    "    Amirabbas Afzali \n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T00:14:59.623698Z",
     "iopub.status.busy": "2024-12-26T00:14:59.623421Z",
     "iopub.status.idle": "2024-12-26T00:14:59.641677Z",
     "shell.execute_reply": "2024-12-26T00:14:59.640880Z",
     "shell.execute_reply.started": "2024-12-26T00:14:59.623678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set your student number\n",
    "student_number = '401110437'\n",
    "Name = 'Parsa'\n",
    "Last_Name = 'Ghezelbash'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules\n",
    "- Make sure that all of your cells can be run perfectly. \n",
    "- Try to minimize your use of ChatGPT (or any other AI assistant) as much as possible.\n",
    "- You must create a report for this task in PDF format and explain the main results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are a class of deep learning models designed for processing and generating natural language. These models are trained using large amounts of textual data and utilize architectures based on transformers. Some of the applications of these models include text generation, machine translation, text summarization, question answering, and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder-Decoder LLMs*\n",
    "\n",
    "One of the common architectures in large language models is the Encoder-Decoder architecture. In this architecture, the encoder processes an input sequence and maps it to a latent space. Then, the decoder uses this latent space to generate an output sequence. Models like T5 [1] (Text-to-Text Transfer Transformer) use this architecture to perform various tasks. In T5, all tasks are expressed in a \"text-to-text\" format, meaning both input and output are text. This model has capabilities such as translation, summarization, and text classification. One of the advantages of the Encoder-Decoder architecture is that it allows the encoder to utilize information from both before and after a word to gain a more comprehensive understanding of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder-only LLMs*\n",
    "\n",
    "Decoder-only models, such as GPT-2, GPT-3, and LLaMA [2], unlike the Encoder-Decoder architecture, only use the decoder part. These models use an autoregressive mode, meaning they predict the next token based on previous tokens. These models are highly efficient for text generation and have found widespread applications today.\n",
    "\n",
    "Advantages of Decoder-only Models\n",
    "\n",
    "- Efficiency: Decoder-only models are more efficient than Encoder-Decoder models due to the absence of an independent encoder. This makes them require fewer computational and memory resources.\n",
    "- Simplicity: Due to their autoregressive nature, these models can easily generate sequences in order.\n",
    "- Scalability: Due to their simpler architecture, these models can be scaled to much larger sizes.\n",
    "\n",
    "\n",
    "However, one of the drawbacks of these models is that they can only utilize information from tokens before the current token and cannot use tokens that come after for prediction. This limitation is significant in tasks like classification or translation, where a full understanding of the sequence is needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objective of the Exercise**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, the goal is to convert a generative Decoder-only language model into an encoder and evaluate its performance on a binary sentiment classification task. The main aim is to modify the Decoder-only model so that it can function as an encoder and better handle tasks requiring bidirectional understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **In this exercise, you should:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this exercise, you should:\n",
    "\n",
    "1. **Import a Decoder-only model** and load the weights of a pre-trained version of the model.\n",
    "2. **Generate several outputs from the model**, and include at most 10 sample outputs in your report for different inputs.  \n",
    "   You should also briefly explain the effects of key configurations in text generation, including:  \n",
    "   - `Temperature`\n",
    "   - `top_k`\n",
    "   - `top_p`\n",
    "   - `repetition_penalty`\n",
    "   - `num_beams`\n",
    "   - `no_repeat_ngram_size`\n",
    "3. **Load the SST-2 dataset**, which is part of the GLUE benchmark for sentiment classification.  \n",
    "   - Note that the model’s output depends on the number of input tokens. \n",
    "   - Apply necessary padding to the dataset after loading it to allow for parallel execution of the model.\n",
    "4. **Remove the model’s final layer**, which outputs to the size of the model’s dictionary.  \n",
    "   - Use the embedding vector of the first token (CLS token) for classification.\n",
    "5. As observed in the previous step, sometimes the embedding vector of the first token does not provide a good representation of the entire input text.  \n",
    "   - **Add a linear layer** with the same input and output dimensions on top of the encoder's output, and use the output of this linear layer (corresponding to the CLS token) for classification.  \n",
    "   - This step aggregates information of different tokens to get a comprehensive understanding of the input text.\n",
    "6. **Instead of the linear layer** in the previous section, use a **bidirectional attention layer** with a custom number of heads (preferably 12).\n",
    "7. **Repeat step 6** using **left-to-right unidirectional attention** and **right-to-left unidirectional attention**.\n",
    "8. **Load a pre-trained decoder** (preferably BERT-base) and report its **zero-shot performance** (i.e., without needing to train the model) on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation:**\n",
    "\n",
    "In this exercise, for each of sections 4, 5, 6, 7, and 8, you need to plot the confusion matrix corresponding to the model's performance on the test data. Additionally, you should plot two separate graphs showing the training loss and the accuracy of the trained models, and compare them with each other, providing an appropriate analysis of your results. Also, note that high accuracy is not expected for sections 4 and 5, but the correctness of your code will be checked. However, for sections 6 and 7, higher accuracy (around 90%) is expected.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's go:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `gpt2` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T00:15:18.654920Z",
     "iopub.status.busy": "2024-12-26T00:15:18.654621Z",
     "iopub.status.idle": "2024-12-26T00:15:28.884160Z",
     "shell.execute_reply": "2024-12-26T00:15:28.883424Z",
     "shell.execute_reply.started": "2024-12-26T00:15:18.654897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b9297f75554a27bd9ebcc212ef14e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  21%|##1       | 115M/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Password Please\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Password Please\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad364c121354d9d87ed2599f6c321cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Model, GenerationConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `sst-2` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T00:16:24.670776Z",
     "iopub.status.busy": "2024-12-26T00:16:24.670329Z",
     "iopub.status.idle": "2024-12-26T00:16:31.162435Z",
     "shell.execute_reply": "2024-12-26T00:16:31.161615Z",
     "shell.execute_reply.started": "2024-12-26T00:16:24.670745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf14452c30e4cccbe27e9e5e653bc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Password Please\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Password Please\\.cache\\huggingface\\hub\\datasets--glue. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5775a70e57bf49d18ba7e967c8acba2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f62a25818c40d7ab730950ee430f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c3bfbba9534d5dabb175e2cd628a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47c3ee01294483fbd8ad31b3fd12a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca9d4d5f9834e8499964d95c3256b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a032a4183a64cd884dcbbde69d49652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SST-2 dataset from Hugging Face \n",
    "dataset = load_dataset(\"glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go ahead:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"cls_token\": \"<|CLS|>\"})\n",
    "model.resize_token_embeddings(len(tokenizer), mean_resizing=False)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T00:28:51.650055Z",
     "iopub.status.busy": "2024-12-26T00:28:51.649706Z",
     "iopub.status.idle": "2024-12-26T00:28:53.067431Z",
     "shell.execute_reply": "2024-12-26T00:28:53.066754Z",
     "shell.execute_reply.started": "2024-12-26T00:28:51.650027Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1: Once upon a time\n",
      "----------------------------------------\n",
      "  [Gen config 1] --> Once upon a time, I did not believe that the world was really in its \"perfect state.\" Even though there were many different cultures and religions who believed they could be\n",
      "\n",
      "Prompt 2: Deep learning is all about\n",
      "----------------------------------------\n",
      "  [Gen config 1] --> Deep learning is all about finding the right combination of concepts to build on and make them work for you. Here's how:\n",
      "The first step in understanding a new concept,\n",
      "\n",
      "Prompt 3: In a shocking finding today, scientists discovered\n",
      "----------------------------------------\n",
      "  [Gen config 1] --> In a shocking finding today, scientists discovered that the two species of birds are far more likely to be able and willing than ever before to make their nests in dense vegetation.\n",
      "\n",
      "\n",
      " (Image\n",
      "\n",
      "Prompt 4: The meaning of life is\n",
      "----------------------------------------\n",
      "  [Gen config 1] --> The meaning of life is not as simple and straightforward, so we must distinguish between the two. But there are many interesting parallels that could be drawn from this text:\n",
      "\"\n",
      "\n",
      "Prompt 5: The best way to learn is\n",
      "----------------------------------------\n",
      "  [Gen config 1] --> The best way to learn is by doing your own thing. You can't be good at something unless you're a teacher.\"\n",
      "\n",
      "Prompt 6: The most important thing in the world is\n",
      "----------------------------------------\n",
      "  [Gen config 1] --> The most important thing in the world is to find your own life, and that's why we have a lot of support groups. It may be hard at first but once you get over it\n",
      "\n",
      "Prompt 7: The most beautiful thing in the world is\n",
      "----------------------------------------\n",
      "  [Gen config 1] --> The most beautiful thing in the world is, you know what I mean? You're so excited to be playing this game. The more people who buy it and play with us on social media\n",
      "\n",
      "Prompt 8: The most dangerous thing in the world is\n",
      "----------------------------------------\n",
      "  [Gen config 1] --> The most dangerous thing in the world is that you have a child who's gonna be abused,\" he said.\n",
      "\"You're going to need somebody on your team.\"\n",
      "\n",
      "Prompt 9: The most exciting thing in the world is\n",
      "----------------------------------------\n",
      "  [Gen config 1] --> The most exciting thing in the world is that you get to be part of a team,\" he said. \"You have your own personality and they are all based on one big belief.\"\n",
      "\n",
      "\n",
      "Prompt 10: The most boring thing in the world is\n",
      "----------------------------------------\n",
      "  [Gen config 1] --> The most boring thing in the world is a man who can't see or hear, and whose entire mind lives on something he doesn´t understand. A perfect example of this could be his\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"Deep learning is all about\",\n",
    "    \"In a shocking finding today, scientists discovered\",\n",
    "    \"The meaning of life is\",\n",
    "    \"The best way to learn is\",\n",
    "    \"The most important thing in the world is\",\n",
    "    \"The most beautiful thing in the world is\",\n",
    "    \"The most dangerous thing in the world is\",\n",
    "    \"The most exciting thing in the world is\",\n",
    "    \"The most boring thing in the world is\",\n",
    "]\n",
    "\n",
    "generation_configs = [\n",
    "    GenerationConfig(\n",
    "        max_new_tokens=30,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id = tokenizer.pad_token_id,\n",
    "        num_beams=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        do_sample=True\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts[:10]):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    print(f\"\\nPrompt {i+1}: {prompt}\\n{'-'*40}\")\n",
    "\n",
    "    for j, gen_cfg in enumerate(generation_configs):\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            generation_config=gen_cfg\n",
    "        )\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        print(f\"  [Gen config {j+1}] --> {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ceeb807d114915b0cd6663f3e8430c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42c09fca8884c47a3a8836ea85d4e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a676151c8ab4e2085e199521f03f4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample: {'label': tensor(0), 'input_ids': tensor([50257,  7808,   649,  3200,   507,   422,   262, 21694,  4991,   220,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    texts = [\"<|CLS|> \" + t for t in examples[\"sentence\"]]\n",
    "    return tokenizer(texts, truncation=True, padding=\"max_length\", max_length=32)\n",
    "\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"label\", \"attention_mask\"])\n",
    "\n",
    "train_data = dataset[\"train\"]\n",
    "val_data   = dataset[\"validation\"]\n",
    "test_data  = dataset[\"test\"]\n",
    "\n",
    "print(\"Train sample:\", train_data[0])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_data, batch_size=8)\n",
    "test_loader  = DataLoader(test_data, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS vector shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "model_without_lm_head = GPT2Model.from_pretrained(\"gpt2\")\n",
    "model_without_lm_head.resize_token_embeddings(len(tokenizer))\n",
    "model_without_lm_head.to(device).eval()\n",
    "\n",
    "def extract_cls_embedding(input_ids, attention_mask):\n",
    "    outputs = model_without_lm_head(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "    cls_embed = hidden_states[:, 0, :]\n",
    "    return cls_embed\n",
    "\n",
    "sample = train_data[0]\n",
    "input_ids = sample[\"input_ids\"].unsqueeze(0).to(device)\n",
    "attention_mask = sample[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "\n",
    "cls_vector = extract_cls_embedding(input_ids, attention_mask)\n",
    "print(\"CLS vector shape:\", cls_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ClassifierLinear created.\n"
     ]
    }
   ],
   "source": [
    "class GPT2ClassifierLinear(nn.Module):\n",
    "    def __init__(self, gpt2_model, hidden_dim=768, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.gpt2 = gpt2_model\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        cls_token = hidden_states[:, 0, :]\n",
    "        logits = self.linear(cls_token)\n",
    "        return logits\n",
    "\n",
    "model_cls_linear = GPT2ClassifierLinear(model_without_lm_head).to(device)\n",
    "\n",
    "print(\"GPT2ClassifierLinear created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (logits.argmax(dim=-1) == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    return total_loss / len(loader), total_correct / total_samples\n",
    "\n",
    "def eval_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (logits.argmax(dim=-1) == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(loader), total_correct / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8419/8419 [27:06<00:00,  5.18it/s]\n",
      "Evaluation: 100%|██████████| 109/109 [00:01<00:00, 73.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 -> Train loss: 0.6927, Train acc: 0.5448, Val loss: 0.6996, Val acc: 0.5092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8419/8419 [10:24<00:00, 13.48it/s]\n",
      "Evaluation: 100%|██████████| 109/109 [00:01<00:00, 69.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 -> Train loss: 0.6885, Train acc: 0.5549, Val loss: 0.6950, Val acc: 0.5092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8419/8419 [10:25<00:00, 13.45it/s]\n",
      "Evaluation: 100%|██████████| 109/109 [00:01<00:00, 66.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 -> Train loss: 0.6875, Train acc: 0.5566, Val loss: 0.6970, Val acc: 0.5092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation:   0%|          | 0/228 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m eval_model(model_cls_linear, val_loader, criterion)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_cls_linear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 39\u001b[0m, in \u001b[0;36meval_model\u001b[1;34m(model, loader, criterion)\u001b[0m\n\u001b[0;32m     36\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[0;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m---> 39\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (logits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     41\u001b[0m total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = AdamW(model_cls_linear.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(model_cls_linear, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = eval_model(model_cls_linear, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} -> Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = eval_model(model_cls_linear, test_loader, criterion)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2ClassifierBiAttn(nn.Module):\n",
    "    def __init__(self, gpt2_model, hidden_dim=768, num_labels=2, num_heads=12):\n",
    "        super().__init__()\n",
    "        self.gpt2 = gpt2_model\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        cls_token = hidden_states[:, 0:1, :]\n",
    "\n",
    "        out, _ = self.attn(query=cls_token, key=hidden_states, value=hidden_states)\n",
    "\n",
    "        logits = self.linear(out.squeeze(1))\n",
    "        return logits\n",
    "\n",
    "model_cls_biattn = GPT2ClassifierBiAttn(\n",
    "    gpt2_model=model_without_lm_head, \n",
    "    hidden_dim=model_without_lm_head.config.hidden_size, \n",
    "    num_labels=2, \n",
    "    num_heads=12\n",
    ").to(device)\n",
    "\n",
    "print(\"GPT2ClassifierBiAttn created (bidirectional attention).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model_cls_biattn.parameters(), lr=5e-5)\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(model_cls_biattn, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = eval_model(model_cls_biattn, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} -> Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = eval_model(model_cls_biattn, test_loader, criterion)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_causal_mask(seq_len):\n",
    "    mask = torch.ones(seq_len, seq_len).triu(1).bool()\n",
    "    return mask\n",
    "\n",
    "class GPT2ClassifierCausalAttn(nn.Module):\n",
    "    def __init__(self, gpt2_model, hidden_dim=768, num_labels=2, num_heads=12, l2r=True):\n",
    "        super().__init__()\n",
    "        self.gpt2 = gpt2_model\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, num_labels)\n",
    "        self.l2r = l2r\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt2(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        B, L, H = hidden_states.shape\n",
    "\n",
    "        cls_token = hidden_states[:, 0:1, :]\n",
    "        causal_mask = generate_causal_mask(L).to(hidden_states.device)\n",
    "        \n",
    "        if not self.l2r:\n",
    "            causal_mask = torch.flip(causal_mask, dims=(0,1))\n",
    "        \n",
    "        out, _ = self.attn(\n",
    "            query=cls_token,\n",
    "            key=hidden_states,\n",
    "            value=hidden_states,\n",
    "            attn_mask=causal_mask[:1, :],\n",
    "        )\n",
    "        logits = self.linear(out.squeeze(1))\n",
    "        return logits\n",
    "\n",
    "model_cls_l2r = GPT2ClassifierCausalAttn(\n",
    "    model_without_lm_head, \n",
    "    hidden_dim=model_without_lm_head.config.hidden_size, \n",
    "    num_labels=2,\n",
    "    num_heads=12,\n",
    "    l2r=True\n",
    ").to(device)\n",
    "\n",
    "model_cls_r2l = GPT2ClassifierCausalAttn(\n",
    "    model_without_lm_head,\n",
    "    hidden_dim=model_without_lm_head.config.hidden_size,\n",
    "    num_labels=2,\n",
    "    num_heads=12,\n",
    "    l2r=False\n",
    ").to(device)\n",
    "\n",
    "print(\"GPT2ClassifierCausalAttn created for L2R and R2L unidirectional attention.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model_cls_l2r.parameters(), lr=5e-5)\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(model_cls_l2r, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = eval_model(model_cls_l2r, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} -> Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "    \n",
    "test_loss, test_acc = eval_model(model_cls_l2r, test_loader, criterion)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    " \n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
    "bert_model.eval()\n",
    "\n",
    "print(\"Loaded BERT-base for classification (zero-shot).\")\n",
    "\n",
    "optimizer = AdamW(bert_model.parameters(), lr=5e-5)\n",
    "n_epochs = 3\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss, train_acc = train_epoch(bert_model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = eval_model(bert_model, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} -> Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}, Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "    \n",
    "test_loss, test_acc = eval_model(bert_model, test_loader, criterion)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "### References\n",
    "\n",
    "[1] Raffel, Colin, Noam Shazeer, Adam Roberts, et al. (2020). *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*. [Link to paper](https://arxiv.org/abs/1910.10683)\n",
    "\n",
    "[2] Touvron, Hugo, et al. (2023). *LLaMA 2: Open Foundation and Fine-Tuned Chat Models*. [Link to paper](https://arxiv.org/abs/2307.09288)\n",
    "\n",
    "<span style=\"color:yellow;\">*For further reading on this field of research, you can refer to the following papers:*</span>\n",
    "\n",
    "[3] BehnamGhader, Adlakha, et al. (2024). *LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders*. [Link to paper](https://arxiv.org/abs/2404.05961)\n",
    "\n",
    "[4] Gao, Tianyu, et al. (2021). *SimCSE: Simple Contrastive Learning of Sentence Embeddings*. [Link to paper](https://arxiv.org/abs/2104.08821)\n",
    "\n",
    "[5] Lee, et al. (2023). *NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models*. [Link to paper](https://arxiv.org/abs/2405.17428)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Best regards.**"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "550b73d17028e65cfbd266e0c945d7274f18a7a366e249c5ab11fc4eb0cd2459"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
