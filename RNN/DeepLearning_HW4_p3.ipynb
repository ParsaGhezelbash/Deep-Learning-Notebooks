{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"550b73d17028e65cfbd266e0c945d7274f18a7a366e249c5ab11fc4eb0cd2459"}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<br>\n<font>\n<div dir=ltr align=center>\n<font color=0F5298 size=10>\n    Deep Learning - HW4 <br>\n<font color=2565AE size=5>\n    Electrical Engineering Department <br>\n    winter 2024<br>\n<font color=3C99D size=5>\n    Practical Assignment 3 <br>\n<font color=696880 size=4>\n    Amirabbas Afzali \n\n____","metadata":{}},{"cell_type":"markdown","source":"# Personal Data","metadata":{}},{"cell_type":"code","source":"# Set your student number\nstudent_number = '401110437'\nName = 'Parsa'\nLast_Name = 'Ghezelbash'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T00:14:59.623421Z","iopub.execute_input":"2024-12-26T00:14:59.623698Z","iopub.status.idle":"2024-12-26T00:14:59.641677Z","shell.execute_reply.started":"2024-12-26T00:14:59.623678Z","shell.execute_reply":"2024-12-26T00:14:59.640880Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Rules\n- Make sure that all of your cells can be run perfectly. \n- Try to minimize your use of ChatGPT (or any other AI assistant) as much as possible.\n- You must create a report for this task in PDF format and explain the main results.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## **Introduction**","metadata":{}},{"cell_type":"markdown","source":"Large Language Models (LLMs) are a class of deep learning models designed for processing and generating natural language. These models are trained using large amounts of textual data and utilize architectures based on transformers. Some of the applications of these models include text generation, machine translation, text summarization, question answering, and text classification.","metadata":{}},{"cell_type":"markdown","source":"### *Encoder-Decoder LLMs*\n\nOne of the common architectures in large language models is the Encoder-Decoder architecture. In this architecture, the encoder processes an input sequence and maps it to a latent space. Then, the decoder uses this latent space to generate an output sequence. Models like T5 [1] (Text-to-Text Transfer Transformer) use this architecture to perform various tasks. In T5, all tasks are expressed in a \"text-to-text\" format, meaning both input and output are text. This model has capabilities such as translation, summarization, and text classification. One of the advantages of the Encoder-Decoder architecture is that it allows the encoder to utilize information from both before and after a word to gain a more comprehensive understanding of the text.","metadata":{}},{"cell_type":"markdown","source":"### *Decoder-only LLMs*\n\nDecoder-only models, such as GPT-2, GPT-3, and LLaMA [2], unlike the Encoder-Decoder architecture, only use the decoder part. These models use an autoregressive mode, meaning they predict the next token based on previous tokens. These models are highly efficient for text generation and have found widespread applications today.\n\nAdvantages of Decoder-only Models\n\n- Efficiency: Decoder-only models are more efficient than Encoder-Decoder models due to the absence of an independent encoder. This makes them require fewer computational and memory resources.\n- Simplicity: Due to their autoregressive nature, these models can easily generate sequences in order.\n- Scalability: Due to their simpler architecture, these models can be scaled to much larger sizes.\n\n\nHowever, one of the drawbacks of these models is that they can only utilize information from tokens before the current token and cannot use tokens that come after for prediction. This limitation is significant in tasks like classification or translation, where a full understanding of the sequence is needed.\n\n","metadata":{}},{"cell_type":"markdown","source":"## **Objective of the Exercise**\n","metadata":{}},{"cell_type":"markdown","source":"In this exercise, the goal is to convert a generative Decoder-only language model into an encoder and evaluate its performance on a binary sentiment classification task. The main aim is to modify the Decoder-only model so that it can function as an encoder and better handle tasks requiring bidirectional understanding.","metadata":{}},{"cell_type":"markdown","source":"## **In this exercise, you should:**\n","metadata":{}},{"cell_type":"markdown","source":"### In this exercise, you should:\n\n1. **Import a Decoder-only model** and load the weights of a pre-trained version of the model.\n2. **Generate several outputs from the model**, and include at most 10 sample outputs in your report for different inputs.  \n   You should also briefly explain the effects of key configurations in text generation, including:  \n   - `Temperature`\n   - `top_k`\n   - `top_p`\n   - `repetition_penalty`\n   - `num_beams`\n   - `no_repeat_ngram_size`\n3. **Load the SST-2 dataset**, which is part of the GLUE benchmark for sentiment classification.  \n   - Note that the model’s output depends on the number of input tokens. \n   - Apply necessary padding to the dataset after loading it to allow for parallel execution of the model.\n4. **Remove the model’s final layer**, which outputs to the size of the model’s dictionary.  \n   - Use the embedding vector of the first token (CLS token) for classification.\n5. As observed in the previous step, sometimes the embedding vector of the first token does not provide a good representation of the entire input text.  \n   - **Add a linear layer** with the same input and output dimensions on top of the encoder's output, and use the output of this linear layer (corresponding to the CLS token) for classification.  \n   - This step aggregates information of different tokens to get a comprehensive understanding of the input text.\n6. **Instead of the linear layer** in the previous section, use a **bidirectional attention layer** with a custom number of heads (preferably 12).\n7. **Repeat step 6** using **left-to-right unidirectional attention** and **right-to-left unidirectional attention**.\n8. **Load a pre-trained decoder** (preferably BERT-base) and report its **zero-shot performance** (i.e., without needing to train the model) on the test data.","metadata":{}},{"cell_type":"markdown","source":"## **Evaluation:**\n\nIn this exercise, for each of sections 4, 5, 6, 7, and 8, you need to plot the confusion matrix corresponding to the model's performance on the test data. Additionally, you should plot two separate graphs showing the training loss and the accuracy of the trained models, and compare them with each other, providing an appropriate analysis of your results. Also, note that high accuracy is not expected for sections 4 and 5, but the correctness of your code will be checked. However, for sections 6 and 7, higher accuracy (around 90%) is expected.\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## **Let's go:**\n","metadata":{}},{"cell_type":"markdown","source":"Load `gpt2` model:","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Model\nimport torch\nfrom torch import nn\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T00:15:18.654621Z","iopub.execute_input":"2024-12-26T00:15:18.654920Z","iopub.status.idle":"2024-12-26T00:15:28.884160Z","shell.execute_reply.started":"2024-12-26T00:15:18.654897Z","shell.execute_reply":"2024-12-26T00:15:28.883424Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"490b4ebc008246c8b1b0b5e2be87b756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e852e087d742479b96e5c7f72eb2c1d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ddda121a01b493d93d35b2323f197e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63e5b76eadd44b769b6c9820a96e1acc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f826e85d6e05444ea7fd48fa0dea308c"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3190c1d27c2436c9aa973b68b2edf45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb2993a1781e4a88a9cb6f1b8584589f"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Load `sst-2` dataset:","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the SST-2 dataset from Hugging Face \ndataset = load_dataset(\"glue\", \"sst2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T00:16:24.670329Z","iopub.execute_input":"2024-12-26T00:16:24.670776Z","iopub.status.idle":"2024-12-26T00:16:31.162435Z","shell.execute_reply.started":"2024-12-26T00:16:24.670745Z","shell.execute_reply":"2024-12-26T00:16:31.161615Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bbd38dde17d41ac901aad5d5eed589d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae09ff377961463ba9a4f3f4e6f3c0c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"258b4b995b1a4c45ba95b63830b00954"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75094927b323413e80af3eddd4ba935d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9b07792697436eab188c239fbec8ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a420a31da4a42079e1fe745a1cc1c13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce248d934814a5ebb9646fe82ea9102"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"go ahead:","metadata":{}},{"cell_type":"code","source":"# Example inputs for text generation\nprompts = [\n    \"Artificial intelligence is\",\n    \"The future of AI will\",\n    \"Deep learning models are\",\n]\n\n# Generate text\noutputs = []\nfor prompt in prompts:\n    tokenizer.pad_token = tokenizer.eos_token  # Set pad token as eos token\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n    attention_mask = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).attention_mask\n    generated_ids = model.generate(\n        input_ids=input_ids.to(\"cuda\"),\n        attention_mask=attention_mask.to(\"cuda\"),\n        max_length=50,\n        pad_token_id=tokenizer.pad_token_id,\n        do_sample=True,\n        temperature=0.7,  # Example temperature\n        top_k=50,         # Example top_k\n        top_p=0.9,        # Example top_p\n        repetition_penalty=1.2,\n        num_beams=5,      # Example num_beams\n        no_repeat_ngram_size=2\n    )\n    outputs.append(tokenizer.decode(generated_ids[0], skip_special_tokens=True))\n\n# Print outputs\nfor i, output in enumerate(outputs):\n    print(f\"Prompt: {prompts[i]}\")\n    print(f\"Generated Text: {output}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T00:28:51.649706Z","iopub.execute_input":"2024-12-26T00:28:51.650055Z","iopub.status.idle":"2024-12-26T00:28:53.067431Z","shell.execute_reply.started":"2024-12-26T00:28:51.650027Z","shell.execute_reply":"2024-12-26T00:28:53.066754Z"}},"outputs":[{"name":"stdout","text":"Prompt: Artificial intelligence is\nGenerated Text: Artificial intelligence is one of the most important advances in the field of artificial intelligence, and it has been a major contributor to the advancement of our understanding of life on Earth.\n\nIn this article, we are going to look at a number of\n\nPrompt: The future of AI will\nGenerated Text: The future of AI will be determined by what happens in the next few years,\" he said.\n\nExplore further: Artificial intelligence could change the way we live, work and play\n\nPrompt: Deep learning models are\nGenerated Text: Deep learning models are often used to predict the future.\n\nIn this article, we will explore how to use these models to build predictive models that can be used in real-world situations. We will also look at how we can use them to\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import DataCollatorWithPadding\n\n# Load SST-2 dataset\ndataset = load_dataset(\"glue\", \"sst2\")\n\n# Tokenize and pad\ndef tokenize(batch):\n    return tokenizer(batch[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n\ntokenized_dataset = dataset.map(tokenize, batched=True)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n\n# Prepare DataLoaders\ntrain_loader = DataLoader(tokenized_dataset[\"train\"], batch_size=32, collate_fn=data_collator)\nvalidation_loader = DataLoader(tokenized_dataset[\"validation\"], batch_size=32, collate_fn=data_collator)\ntest_loader = DataLoader(tokenized_dataset[\"test\"], batch_size=32, collate_fn=data_collator)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T00:28:59.924555Z","iopub.execute_input":"2024-12-26T00:28:59.924894Z","iopub.status.idle":"2024-12-26T00:29:22.632011Z","shell.execute_reply.started":"2024-12-26T00:28:59.924832Z","shell.execute_reply":"2024-12-26T00:29:22.631297Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e0a717800cb4834ad23b7fd2c32ca0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2efd0759241c46c38908786c620221f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b29b526cfec40cf88c44e3c9bad087d"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from transformers import GPT2Model\n\n# Load GPT-2 without the language modeling head\nbase_model = GPT2Model.from_pretrained(\"gpt2\").to(\"cuda\")\n\n# Extract CLS token embedding\ndef extract_cls_embedding(batch):\n    inputs = batch[\"input_ids\"].to(\"cuda\")\n    attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n    outputs = base_model(input_ids=inputs, attention_mask=attention_mask)\n    cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n    return cls_embedding\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T00:29:49.930441Z","iopub.execute_input":"2024-12-26T00:29:49.930724Z","iopub.status.idle":"2024-12-26T00:29:50.378752Z","shell.execute_reply.started":"2024-12-26T00:29:49.930702Z","shell.execute_reply":"2024-12-26T00:29:50.378130Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class GPT2Classifier(nn.Module):\n    def __init__(self, hidden_dim, num_labels):\n        super().__init__()\n        self.linear = nn.Linear(hidden_dim, num_labels)\n    \n    def forward(self, cls_embedding):\n        return self.linear(cls_embedding)\n\n# Example: Add linear layer\nclassifier = GPT2Classifier(hidden_dim=base_model.config.hidden_size, num_labels=2).to(\"cuda\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T00:30:19.380120Z","iopub.execute_input":"2024-12-26T00:30:19.380424Z","iopub.status.idle":"2024-12-26T00:30:19.386629Z","shell.execute_reply.started":"2024-12-26T00:30:19.380402Z","shell.execute_reply":"2024-12-26T00:30:19.385929Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class BiAttention(nn.Module):\n    def __init__(self, hidden_dim, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n\n    def forward(self, embeddings):\n        attn_output, _ = self.attention(embeddings, embeddings, embeddings)\n        return attn_output[:, 0, :]  # CLS token output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class UnidirectionalAttention(nn.Module):\n    def __init__(self, hidden_dim, num_heads, direction=\"left-to-right\"):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n        self.direction = direction\n\n    def forward(self, embeddings):\n        seq_len = embeddings.size(1)\n        mask = torch.tril(torch.ones(seq_len, seq_len)) if self.direction == \"left-to-right\" else torch.triu(torch.ones(seq_len, seq_len))\n        mask = mask.to(embeddings.device).unsqueeze(0)\n        attn_output, _ = self.attention(embeddings, embeddings, embeddings, attn_mask=mask)\n        return attn_output[:, 0, :]  # CLS token output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\n\n# Load pre-trained BERT\nbert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(\"cuda\")\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Zero-shot evaluation\ndef evaluate_bert(batch):\n    inputs = bert_tokenizer(batch[\"sentence\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(\"cuda\")\n    outputs = bert_model(**inputs)\n    predictions = torch.argmax(outputs.logits, dim=1)\n    return predictions\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---------\n### References\n\n[1] Raffel, Colin, Noam Shazeer, Adam Roberts, et al. (2020). *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer*. [Link to paper](https://arxiv.org/abs/1910.10683)\n\n[2] Touvron, Hugo, et al. (2023). *LLaMA 2: Open Foundation and Fine-Tuned Chat Models*. [Link to paper](https://arxiv.org/abs/2307.09288)\n\n<span style=\"color:yellow;\">*For further reading on this field of research, you can refer to the following papers:*</span>\n\n[3] BehnamGhader, Adlakha, et al. (2024). *LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders*. [Link to paper](https://arxiv.org/abs/2404.05961)\n\n[4] Gao, Tianyu, et al. (2021). *SimCSE: Simple Contrastive Learning of Sentence Embeddings*. [Link to paper](https://arxiv.org/abs/2104.08821)\n\n[5] Lee, et al. (2023). *NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models*. [Link to paper](https://arxiv.org/abs/2405.17428)\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Best regards.**","metadata":{}}]}