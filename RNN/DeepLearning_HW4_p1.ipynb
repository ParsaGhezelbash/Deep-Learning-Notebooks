{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp7D0ktn5eiG"
      },
      "source": [
        "\n",
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=10>\n",
        "    Deep Learning - HW4 <br>\n",
        "<font color=2565AE size=5>\n",
        "    Electrical Engineering Department <br>\n",
        "    winter 2024<br>\n",
        "<font color=3C99D size=5>\n",
        "    Practical Assignment 1 <br>\n",
        "<font color=696880 size=4>\n",
        "    Niloufar Abbasi \n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Emotion Recognition with Recurrent Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## **Introduction**\n",
        "\n",
        "In this task, you will implement a Recurrent Neural Network (RNN) to perform tweet emotion recognition. The goal is to classify tweets into one of six basic emotions: \n",
        "- **0: sadness**\n",
        "- **1: joy**\n",
        "- **2: love**\n",
        "- **3: anger**\n",
        "- **4: fear**\n",
        "- **5: surprise**\n",
        "\n",
        "We will be using a dataset of tweets, where each tweet has been labeled with one of these emotions. Throughout this notebook, you will follow step-by-step instructions to build and train your model. As you progress, you'll complete the provided TODO sections by filling in the necessary code and functions.\n",
        "\n",
        "By the end of this exercise, you will:\n",
        "- Understand how to preprocess text data for use in RNN models.\n",
        "- Build and train a model using Pytorch.\n",
        "- Evaluate the performance of your model on the test set.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 1: Enter Your Information\n",
        "\n",
        "Before we begin, please enter your **student ID** and **name** in the code section below.\n",
        "\n",
        "Make sure to replace the placeholders with your actual information.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#########################################\n",
        "# TODO: Enter your personal information #\n",
        "#########################################\n",
        "\n",
        "student_name = ...  # Replace with your name\n",
        "student_id   = ...  # Replace with your student ID\n",
        "\n",
        "print(\"Your name:\", student_name)\n",
        "print(\"Your student ID:\", student_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 2: Setup and Imports\n",
        "\n",
        "In this task, we'll be preparing our environment by importing the necessary libraries. Please run the cells below to set everything up. If any library is missing on your end, make sure to install it.\n",
        "\n",
        "### Instructions:\n",
        "1. **Run the cells** to install any required packages and import the necessary libraries.\n",
        "2. If any library is not installed in your environment, use `!pip install <library_name>` to install it.\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Required Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from datasets import load_dataset\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#####################################################################\n",
        "# TODO: Add any additional libraries you may need for your analysis #\n",
        "#####################################################################\n",
        " \n",
        "# You can add your libraries here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional Functions\n",
        "\n",
        "The following functions have been provided to help you visualize the results of your model training. These will allow you to analyze both the model's performance over time (through the training history) and how well it performs on individual predictions (through a confusion matrix).\n",
        "\n",
        "1. **`plot_history(hist)`**: This function takes the training history object (`hist`) from your model and plots both the accuracy and loss over the number of epochs for both training and validation sets. It provides an easy way to visually track the model's learning progress and performance.\n",
        "\n",
        "2. **`visualize_confusion_matrix(actual_labels, predicted_labels, classes)`**: After making predictions with your model, you can use this function to plot a confusion matrix. It shows the accuracy of your model across each class, helping to identify areas where the model is performing well and where it might be struggling. The matrix is normalized to make comparisons easier.\n",
        "\n",
        "These functions are for your convenience, you can directly use them after running your model to visualize the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Functions to visualize training history and confusion matrix\n",
        "def plot_history(hist):\n",
        "    epochs = len(hist.history['loss'])\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(0, epochs), hist.history.get('accuracy'), label='Training')\n",
        "    plt.plot(range(0, epochs), hist.history.get('val_accuracy'), label='Validation')\n",
        "    plt.ylim([0., 1.])\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(0, epochs), hist.history.get('loss'), label='Training')\n",
        "    plt.plot(range(0, epochs), hist.history.get('val_loss'), label='Validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_confusion_matrix(actual_labels, predicted_labels, classes):\n",
        "\n",
        "    cm = confusion_matrix(actual_labels, predicted_labels, normalize='true')\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    sp = plt.subplot(1, 1, 1)\n",
        "    ctx = sp.matshow(cm)\n",
        "    plt.xticks(list(range(0, 6)), labels=classes)\n",
        "    plt.yticks(list(range(0, 6)), labels=classes)\n",
        "    plt.colorbar(ctx)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 3: Importing Data\n",
        "\n",
        "In this task, we will\n",
        "\n",
        "- import the Tweet Emotion dataset\n",
        "\n",
        "- create training, validation, and test sets\n",
        "    \n",
        "- extract the tweets and their corresponding labels for further analysis\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YHOvjAu5eiL"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('emotion')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s0h541FxIgc"
      },
      "outputs": [],
      "source": [
        "# Display the first few samples\n",
        "print(dataset['train'][0:5])  # Adjust 'train' to 'test' or other splits if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7eCnxU25eiN"
      },
      "outputs": [],
      "source": [
        "###########################################################################\n",
        "# TODO: Create variables for the training, validation, and test datasets. #\n",
        "###########################################################################\n",
        "\n",
        "training = ...\n",
        "validation = ...\n",
        "test = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDYXMfZy5eiP"
      },
      "outputs": [],
      "source": [
        "########################################################################################################\n",
        "# TODO:                                                                                                #\n",
        "# Implement the `extract_tweets_labels` function to iterate over the provided dataset.                 #\n",
        "# Within this function, extract the tweet text and their corresponding labels into two separate lists. #\n",
        "# Then, call this function with the training dataset to obtain the tweets and labels,                  #\n",
        "# ensuring proper data handling for subsequent analysis.                                               #\n",
        "########################################################################################################\n",
        "\n",
        "def extract_tweets_labels(data):\n",
        "    # Extract the text and labels from the data\n",
        "    tweets = ...\n",
        "    labels = ...\n",
        "    return tweets, labels\n",
        "\n",
        "# Get the tweets and labels from the training set\n",
        "tweets, labels = ...\n",
        "\n",
        "# Display the first tweet and its label\n",
        "print(tweets[0], labels[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 4: Tokenization of Tweets\n",
        "\n",
        "In this section, we will focus on the tokenization of the tweets in the dataset. Tokenization is a crucial step in natural language processing (NLP) that involves breaking down text into smaller units, such as words or subwords. This process enables the model to better understand and analyze the text data.\n",
        "\n",
        "### Objectives:\n",
        "1. Implement a tokenizer to convert the tweet texts into tokens.\n",
        "2. Ensure the tokenization process handles various aspects of the text, such as punctuation and special characters.\n",
        "\n",
        "By the end of this task, you will have a set of tokenized tweets ready for further processing in your NLP pipeline.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qfX5-ResxIgq"
      },
      "outputs": [],
      "source": [
        "###############################################################################################################################################\n",
        "# TODO: Tokenization of Tweets                                                                                                                #\n",
        "# 1. Select a Tokenization Method: Choose a suitable tokenization method to preprocess the tweets.                                            #                                 \n",
        "# Ensure that the tokenization handles special characters and maintains the integrity of the text data.                                       #\n",
        "# 2. Implement the Tokenizer: Once you have selected a method, implement the tokenizer to convert the tweet texts into sequences of tokens.   #\n",
        "# 3. Test the Tokenization:                                                                                                                   #\n",
        "# Verify the functionality of your tokenizer by applying it to a sample tweet from the dataset and displaying the resulting tokenized output. #\n",
        "###############################################################################################################################################\n",
        "\n",
        "\n",
        "tokenizer = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 5: Padding and Truncating Sequences\n",
        "\n",
        "In this task, we will prepare the tokenized tweets for model training by checking their lengths and creating padded sequences.\n",
        "\n",
        "1. **Checking the Length of the Tweets**\n",
        "   - Analyze the distribution of tweet lengths to understand the range of token counts in the dataset.\n",
        "\n",
        "2. **Creating Padded Sequences**\n",
        "   - After tokenizing the tweets, sequences of different lengths must be adjusted to a uniform length to facilitate training.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#####################################################################################################################################################\n",
        "# TODO:                                                                                                                                             #\n",
        "# Implement code to calculate the lengths of the tweets and visualize the distribution using a histogram. Use an appropriate visualization library. #\n",
        "# After visualizing the distribution, determine a suitable maximum length for padding and truncating the sequences based on your findings.          #\n",
        "#####################################################################################################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "############################################################################################################\n",
        "# TODO: Define a function to convert the tokenized tweets into sequences and apply padding and truncation. # \n",
        "# Use your determined maximum length for padding the sequences.                                            #\n",
        "############################################################################################################\n",
        "\n",
        "def get_sequences(tokenizer, tweets):\n",
        "    sequences = ...\n",
        "    padded_sequences = ...\n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "padded_train_sequences = get_sequences(tokenizer, tweets)\n",
        "padded_train_sequences[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BURhOX_KxIg8"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 6: Analyzing Label Distribution\n",
        "\n",
        "1. Checking the distribution of labels.\n",
        "2. Addressing any class imbalance using appropriate techniques.\n",
        "\n",
        "Note:\n",
        "\n",
        "(**Providing a clear explanation of the chosen method to address the imbalance and how it will improve the model’s performance is critical.** Without handling class imbalance properly, the model could achieve a high accuracy by focusing on the majority class, but it may perform poorly on the minority class, which would result in misleading metrics and reduced generalization.\n",
        "\n",
        "Hence, it is important to not only implement an appropriate solution but also **justify** why the selected approach is effective for the given task.)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SufT2bpD5ejE"
      },
      "outputs": [],
      "source": [
        "##########################################################################################################################\n",
        "# TODO: Analyze the distribution of labels in the training set. (visualize it in a perfect way)                          #\n",
        "# Determine if there is any class imbalance.                                                                             #\n",
        "# If you observe any imbalance, apply one of the techniques you have learned throughout the course to address the issue. #\n",
        "##########################################################################################################################\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-v0Mnh8xIhP"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 7: Creating the Model\n",
        "\n",
        "Design and implement a recurrent neural network for the task. You may utilize a sequential architecture like RNN or GRU to enhance the model's performance.\n",
        "\n",
        "Build and compile your model using Pytorch.\n",
        "\n",
        "**Once the model is created, summarize its structure to verify the design.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpewXxPQ5eji"
      },
      "outputs": [],
      "source": [
        "#########################################################################################################\n",
        "# TODO:                                                                                                 #\n",
        "# Create a recurrent neural network model for the task.                                                 #\n",
        "# (Suggestion: Implement a Bidirectional LSTM architecture to process the input sequences effectively.) #\n",
        "# Once the model is created, summarize its structure to verify the design.                              #\n",
        "#########################################################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HST_CHjxIhR"
      },
      "source": [
        "---\n",
        "\n",
        "## Task 8: Training the Model\n",
        "\n",
        "In this task, you will prepare a validation set and train your model using the training data.\n",
        "\n",
        "1. Prepare the validation set by extracting tweets and labels, and convert them into sequences using the tokenizer.\n",
        "2. Train the model on the training data.\n",
        "3. Ensure that you implement **Early Stopping** to prevent overfitting and enhance efficiency.\n",
        "\n",
        "Make sure to monitor the validation accuracy during training and adjust the parameters as necessary.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff7F3hCK5ejm"
      },
      "outputs": [],
      "source": [
        "###############################################################################################################\n",
        "# TODO: Extract the tweets and labels from the validation set. Convert the tweets into sequences using the    #\n",
        "#       tokenizer defined earlier. Prepare the data for validation and ensure it matches the format of the    #\n",
        "#       training set.                                                                                         #\n",
        "###############################################################################################################\n",
        "\n",
        "# Your code for extracting and processing validation data goes here\n",
        "\n",
        "###############################################################################################################\n",
        "# TODO: Train the model on the prepared training data. Ensure you implement early stopping to monitor the     #\n",
        "#       validation accuracy and prevent overfitting. Adjust the number of epochs or other hyperparameters as  #\n",
        "#       needed to optimize model performance.                                                                 #\n",
        "###############################################################################################################\n",
        "\n",
        "# Your code for training the model with early stopping goes here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 9: Evaluating the Model\n",
        "\n",
        "In this task, you will evaluate the performance of your model on the test set and visualize the training history. You will also analyze the model’s predictions on both individual and complete test samples.\n",
        "\n",
        "### Steps:\n",
        "1. Visualize the loss and accuracy over the training epochs.\n",
        "2. Prepare and preprocess the test data using functions from Task 2.\n",
        "3. Evaluate the model on the test set and examine individual predictions.\n",
        "4. Analyze the overall predictions and display a confusion matrix.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENCfvXeLxIhX"
      },
      "outputs": [],
      "source": [
        "###############################################################################################################\n",
        "# TODO: Visualize the training history by plotting the loss and accuracy over the epochs to assess model      #\n",
        "#       performance. Use an appropriate visualization method to ensure clarity and effectiveness.             #\n",
        "#       you can use function implemented in Task 2 for your visualizations                                    #\n",
        "###############################################################################################################\n",
        "\n",
        "# Your code for visualizing the training history goes here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWuzoz8uxIha"
      },
      "outputs": [],
      "source": [
        "###############################################################################################################\n",
        "# TODO: Prepare the test set by extracting the tweets and labels, then convert the tweets into sequences using #\n",
        "#       the tokenizer. Ensure the format matches that of the training and validation sets.                     #\n",
        "###############################################################################################################\n",
        "\n",
        "# Your code for preparing the test set goes here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vRVJ_2SxIhc"
      },
      "outputs": [],
      "source": [
        "###############################################################################################################\n",
        "# TODO: Evaluate the model's performance on the test set. Print the accuracy and loss, and review individual   #\n",
        "#       predictions to understand how the model is performing on unseen data.                                 #\n",
        "###############################################################################################################\n",
        "\n",
        "# Your code for evaluating the model and reviewing predictions goes here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh638vHG5ej6"
      },
      "outputs": [],
      "source": [
        "###############################################################################################################\n",
        "# TODO: Generate predictions for all test samples and analyze the model's overall performance. Display the     #\n",
        "#       confusion matrix to visualize the differences between true and predicted labels, helping to interpret  #\n",
        "#       the model's classification results.                                                                   #\n",
        "###############################################################################################################\n",
        "\n",
        "# Your code for generating predictions and displaying the confusion matrix goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Analysis\n",
        "\n",
        "After visualizing the confusion matrix based on your model’s predictions, analyze the results and provide insights into the model’s classification performance.\n",
        "\n",
        "### **Question:**\n",
        "<span style=\"font-size: 1.2em;\">Which class seems to get confused with the class *love* the most?</span>\n",
        "\n",
        "**Remember the class labels:**\n",
        "- **0:** *Sadness*\n",
        "- **1:** *Joy*\n",
        "- **2:** *Love*\n",
        "- **3:** *Anger*\n",
        "- **4:** *Fear*\n",
        "- **5:** *Surprise*\n",
        "\n",
        "Reflect on the confusion matrix to answer this question and provide your reasoning for why this confusion might occur based on the emotional nature of the classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHl5SVCFxIhh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Best regards.**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
