{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import time\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "\n",
    "from utils import (\n",
    "    accuracy,\n",
    "    train,\n",
    "    combined_train,\n",
    "    test,\n",
    "    plot_acc,\n",
    "    plot_loss,\n",
    "    plot_confusion_matrix,\n",
    "    save_model,\n",
    "    get_feature_maps,\n",
    "    visualize_feature_maps,\n",
    "    plot_images,\n",
    "    plot_feature_maps\n",
    ")\n",
    "\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CIFAR10 + Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncode:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, label):\n",
    "        return F.one_hot(torch.tensor(label), num_classes=self.num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 50000\n",
    "\n",
    "val_size = 10000\n",
    "\n",
    "test_size = 10000\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "torch.manual_seed(13)\n",
    "np.random.seed(13)\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "\n",
    "\n",
    "label_transform = OneHotEncode(num_classes=num_classes)\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=label_transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=label_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting <kbd>airplane</kbd> and <kbd>automobile</kbd>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_dataset.classes\n",
    "automobile_index = train_dataset.class_to_idx['automobile']\n",
    "airplane_index = train_dataset.class_to_idx['airplane']\n",
    "\n",
    "\n",
    "print(classes)\n",
    "print(f\"automobile: {automobile_index}\")\n",
    "print(f\"airplane: {airplane_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [airplane_index, automobile_index]\n",
    "\n",
    "classes = np.array(classes)[indices].tolist()\n",
    "\n",
    "train_indices = np.where(np.isin(train_dataset.targets, indices))[0]\n",
    "test_indices = np.where(np.isin(test_dataset.targets, indices))[0]\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "test_subset = Subset(test_dataset, test_indices)\n",
    "\n",
    "train_subset, val_subset = random_split(train_subset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'device: {device.type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, embedding_size=128, num_classes=10):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.embedding = nn.Linear(num_features, embedding_size)\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        embeddings = nn.functional.normalize(self.embedding(features), p=2, dim=1)\n",
    "        class_scores = self.classifier(features)\n",
    "        return embeddings, class_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Loading Pre-Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 128\n",
    "model = CombinedModel(num_embeddings=num_embeddings ,num_classes=num_classes)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ResNet50 for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "epochs = 2\n",
    "\n",
    "train_losses, train_accuracies, validation_losses, validation_accuracies = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterions=criterion,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    log=True,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"time: {np.round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_losses, validation_losses, epochs=epochs, title='Cross Entropy Loss vs epoch for Resnet50')\n",
    "\n",
    "plot_acc(train_accuracies, validation_accuracies, epochs=epochs, title='Accuracy vs epoch for Resnet50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "image, label = images[0], classes[torch.argmax(labels[0])]\n",
    "plot_images([image], [label], title='Feature Map Sample')\n",
    "\n",
    "plot_feature_maps(model, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, triplets):\n",
    "        self.dataset = dataset\n",
    "        self.triplets = triplets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_idx, positive_idx, negative_idx = self.triplets[idx]\n",
    "        anchor, label = self.dataset[anchor_idx]\n",
    "        positive, _ = self.dataset[positive_idx]\n",
    "        negative, _ = self.dataset[negative_idx]\n",
    "        return (anchor, positive, negative), label\n",
    "\n",
    "\n",
    "def make_triplets(dataset):\n",
    "    triplets = []\n",
    "    class_indices = {}\n",
    "\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        c = classes[torch.argmax(label)]\n",
    "        if c not in class_indices:\n",
    "            class_indices[c] = []\n",
    "        class_indices[c].append(idx)\n",
    "\n",
    "    for c in class_indices:\n",
    "        for idx in class_indices[c]:\n",
    "            anchor_idx = idx\n",
    "\n",
    "            positive_idx = np.random.choice([i for i in class_indices[c] if i != anchor_idx])\n",
    "\n",
    "            negative_class = np.random.choice([label for label in class_indices.keys() if label != c])\n",
    "            negative_idx = np.random.choice(class_indices[negative_class])\n",
    "\n",
    "            triplets.append((anchor_idx, positive_idx, negative_idx))\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_triplets = make_triplets(train_subset)\n",
    "val_triplets = make_triplets(val_subset)\n",
    "test_triplets = make_triplets(test_subset)\n",
    "\n",
    "train_triplet_dataset = TripletDataset(train_subset, trian_triplets)\n",
    "val_triplet_dataset = TripletDataset(val_subset, val_triplets)\n",
    "test_triplet_dataset = TripletDataset(test_subset, test_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_triplet_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_triplet_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_triplet_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ResNet50 for Feature Extracting + Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 128\n",
    "triplet_model = CombinedModel(num_embeddings=num_embeddings ,num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model.to(device)\n",
    "criterion = nn.TripletMarginLoss(margin=1, p=2)\n",
    "optimizer = optim.Adam(triplet_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "epochs = 1\n",
    "\n",
    "train_losses, train_accuracies, validation_losses, validation_accuracies = combined_train(\n",
    "    model=triplet_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterions=criterion,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    log=True,\n",
    "    mode='triplet'\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"time: {np.round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_losses, validation_losses, epochs=epochs, title='Triplet Loss vs epoch for Feature Extrator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ResNet50 for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in triplet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in triplet_model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(triplet_model.classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "epochs = 2\n",
    "\n",
    "train_losses, train_accuracies, validation_losses, validation_accuracies = combined_train(\n",
    "    model=triplet_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterions=criterion,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    log=True,\n",
    "    mode='default'\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"time: {np.round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_losses, validation_losses, epochs=epochs, title='Cross Entropy Loss vs epoch for Resnet50 + Feature Extractor')\n",
    "\n",
    "plot_acc(train_accuracies, validation_accuracies, epochs=epochs, title='Accuracy vs epoch for Resnet50 + Feature Extractor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, preds, labels = test(triplet_model, test_loader, criterion, device)\n",
    "\n",
    "print(f'loss= {np.round(loss, 3)}, accuracy= {np.round(acc, 3)}')\n",
    "\n",
    "plot_confusion_matrix(labels, preds,\n",
    "                      class_names=classes, title='combined model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Conclusion and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Loss Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 128\n",
    "combined_model = CombinedModel(num_embeddings=num_embeddings ,num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.to(device)\n",
    "criterion = nn.TripletMarginLoss(margin=1, p=2)\n",
    "optimizer = optim.Adam(combined_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "epochs = 2\n",
    "\n",
    "train_losses, train_accuracies, validation_losses, validation_accuracies = combined_train(\n",
    "    model=combined_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterions=criterion,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    log=True,\n",
    "    mode='combined'\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"time: {np.round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, preds, labels = test(combined_model, test_loader, criterion, device)\n",
    "\n",
    "print(f'loss= {np.round(loss, 3)}, accuracy= {np.round(acc, 3)}')\n",
    "\n",
    "plot_confusion_matrix(labels, preds,\n",
    "                      class_names=classes, title='combined model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
