{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import time\n",
    "\n",
    "import importlib\n",
    "import utils.utils as utils\n",
    "\n",
    "importlib.reload(utils)\n",
    "from utils.utils import (\n",
    "    accuracy,\n",
    "    train,\n",
    "    combined_train,\n",
    "    test,\n",
    "    combined_test,\n",
    "    plot_acc,\n",
    "    plot_loss,\n",
    "    plot_confusion_matrix,\n",
    "    save_model,\n",
    "    get_feature_maps,\n",
    "    visualize_feature_maps,\n",
    "    plot_images,\n",
    "    plot_feature_maps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CIFAR10 + Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncode:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, label):\n",
    "        return F.one_hot(torch.tensor(label), num_classes=self.num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 50000\n",
    "val_size = 10000\n",
    "test_size = 10000\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "num_classes = 2\n",
    "\n",
    "torch.manual_seed(13)\n",
    "np.random.seed(13)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)]\n",
    ")\n",
    "\n",
    "label_transform = OneHotEncode(num_classes=num_classes)\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=label_transform,\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    target_transform=label_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting <kbd>airplane</kbd> and <kbd>automobile</kbd>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_dataset.classes\n",
    "automobile_index = train_dataset.class_to_idx[\"automobile\"]\n",
    "airplane_index = train_dataset.class_to_idx[\"airplane\"]\n",
    "\n",
    "\n",
    "print(classes)\n",
    "print(f\"automobile: {automobile_index}\")\n",
    "print(f\"airplane: {airplane_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [airplane_index, automobile_index]\n",
    "classes = np.array(classes)[indices].tolist()\n",
    "\n",
    "train_indices = np.where(np.isin(train_dataset.targets, indices))[0]\n",
    "test_indices = np.where(np.isin(test_dataset.targets, indices))[0]\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "test_subset = Subset(test_dataset, test_indices)\n",
    "\n",
    "train_subset, val_subset = random_split(train_subset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, embedding_size=128, num_classes=10):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.embedding = nn.Linear(num_features, embedding_size)\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        embeddings = nn.functional.normalize(self.embedding(features), p=2, dim=1)\n",
    "        class_scores = self.classifier(features)\n",
    "        return embeddings, class_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ResNet50 for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "epochs = 30\n",
    "\n",
    "train_losses, train_accuracies, validation_losses, validation_accuracies = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    epochs=epochs,\n",
    "    log=True,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"time: {np.round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(\n",
    "    train_losses,\n",
    "    validation_losses,\n",
    "    epochs=epochs,\n",
    "    title=\"Cross Entropy Loss vs epoch for Resnet50\",\n",
    ")\n",
    "plot_acc(\n",
    "    train_accuracies,\n",
    "    validation_accuracies,\n",
    "    epochs=epochs,\n",
    "    title=\"Accuracy vs epoch for Resnet50\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "image, label = images[0], classes[torch.argmax(labels[0])]\n",
    "plot_images([image], mean, std, [label], title=\"Feature Map Sample\")\n",
    "\n",
    "plot_feature_maps(model, image, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, triplets):\n",
    "        self.dataset = dataset\n",
    "        self.triplets = triplets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_idx, positive_idx, negative_idx = self.triplets[idx]\n",
    "        anchor, label = self.dataset[anchor_idx]\n",
    "        positive, _ = self.dataset[positive_idx]\n",
    "        negative, _ = self.dataset[negative_idx]\n",
    "        return (anchor, positive, negative), label\n",
    "\n",
    "\n",
    "def make_triplets(dataset):\n",
    "    triplets = []\n",
    "    class_indices = {}\n",
    "\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        c = classes[torch.argmax(label)]\n",
    "        if c not in class_indices:\n",
    "            class_indices[c] = []\n",
    "        class_indices[c].append(idx)\n",
    "\n",
    "    for c in class_indices:\n",
    "        for idx in class_indices[c]:\n",
    "            anchor_idx = idx\n",
    "\n",
    "            positive_idx = np.random.choice(\n",
    "                [i for i in class_indices[c] if i != anchor_idx]\n",
    "            )\n",
    "\n",
    "            negative_class = np.random.choice(\n",
    "                [label for label in class_indices.keys() if label != c]\n",
    "            )\n",
    "            negative_idx = np.random.choice(class_indices[negative_class])\n",
    "\n",
    "            triplets.append((anchor_idx, positive_idx, negative_idx))\n",
    "\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_triplets = make_triplets(train_subset)\n",
    "val_triplets = make_triplets(val_subset)\n",
    "test_triplets = make_triplets(test_subset)\n",
    "\n",
    "train_triplet_dataset = TripletDataset(train_subset, trian_triplets)\n",
    "val_triplet_dataset = TripletDataset(val_subset, val_triplets)\n",
    "test_triplet_dataset = TripletDataset(test_subset, test_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_triplet_dataset, batch_size=64, shuffle=True, num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(val_triplet_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(\n",
    "    test_triplet_dataset, batch_size=64, shuffle=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ResNet50 for Feature Extracting + Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 128\n",
    "triplet_model = CombinedModel(embedding_size=num_embeddings, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model.to(device)\n",
    "criterion = nn.TripletMarginLoss(margin=1, p=2)\n",
    "optimizer = optim.Adam(triplet_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "epochs = 30\n",
    "\n",
    "train_losses, train_accuracies, validation_losses, validation_accuracies = (\n",
    "    combined_train(\n",
    "        model=triplet_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        log=True,\n",
    "        mode=\"triplet\",\n",
    "    )\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"time: {np.round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(\n",
    "    train_losses,\n",
    "    validation_losses,\n",
    "    epochs=epochs,\n",
    "    title=\"Triplet Loss vs epoch for Feature Extrator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ResNet50 for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in triplet_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in triplet_model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(triplet_model.classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "epochs = 30\n",
    "\n",
    "train_losses, train_accuracies, validation_losses, validation_accuracies = (\n",
    "    combined_train(\n",
    "        model=triplet_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        log=True,\n",
    "        mode=\"default\",\n",
    "    )\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"time: {np.round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(\n",
    "    train_losses,\n",
    "    validation_losses,\n",
    "    epochs=epochs,\n",
    "    title=\"Cross Entropy Loss vs epoch for Resnet50 + Feature Extractor\",\n",
    ")\n",
    "plot_acc(\n",
    "    train_accuracies,\n",
    "    validation_accuracies,\n",
    "    epochs=epochs,\n",
    "    title=\"Accuracy vs epoch for Resnet50 + Feature Extractor\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, preds, labels = combined_test(\n",
    "    triplet_model, test_loader, criterion, device, mode=\"default\"\n",
    ")\n",
    "print(f\"loss= {np.round(loss, 3)}, accuracy= {np.round(acc, 3)}\")\n",
    "plot_confusion_matrix(labels, preds, class_names=classes, title=\"triplet model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Conclusion and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Loss Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 128\n",
    "combined_model = CombinedModel(embedding_size=num_embeddings, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_model.to(device)\n",
    "criterion = [nn.CrossEntropyLoss(), nn.TripletMarginLoss(margin=1, p=2)]\n",
    "optimizer = optim.Adam(combined_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "epochs = 30\n",
    "\n",
    "train_losses, train_accuracies, validation_losses, validation_accuracies = (\n",
    "    combined_train(\n",
    "        model=combined_model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        epochs=epochs,\n",
    "        log=True,\n",
    "        mode=\"combined\",\n",
    "    )\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"time: {np.round(end_time - start_time, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, preds, labels = combined_test(\n",
    "    combined_model, test_loader, criterion, device, mode=\"combined\"\n",
    ")\n",
    "print(f\"loss= {np.round(loss, 3)}, accuracy= {np.round(acc, 3)}\")\n",
    "plot_confusion_matrix(labels, preds, class_names=classes, title=\"combined model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
