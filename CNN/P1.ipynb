{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CIFAR-10 Dataset + Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncode:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, label):\n",
    "        return F.one_hot(torch.tensor(label), num_classes=self.num_classes).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 50000\n",
    "val_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "torch.manual_seed(13)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "label_transform = OneHotEncode(num_classes=10)\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform,\n",
    "    target_transform=label_transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_subset = Subset(train_dataset, range(train_size))\n",
    "train_subset, val_subset = random_split(train_dataset, [train_size-val_size, val_size])\n",
    "test_subset = Subset(test_dataset, range(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_dataset.classes\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "\n",
    "for image, label in train_subset:\n",
    "    label = torch.argmax(label).item()\n",
    "    if label not in samples:\n",
    "        samples[label] = image\n",
    "    if len(samples) == len(classes):\n",
    "        break\n",
    "\n",
    "max_row = 5\n",
    "\n",
    "fig, axes = plt.subplots(int(len(samples)/max_row), max_row, figsize=[10, 6])\n",
    "\n",
    "keys = np.sort(list(samples.keys()))\n",
    "for i, label in enumerate(keys):\n",
    "    class_name = classes[i]\n",
    "    sample = samples[i] * 0.5 + 0.5\n",
    "    sample = sample.numpy().transpose(1, 2, 0)\n",
    "\n",
    "    ax = axes[int(i / max_row), i % max_row]\n",
    "\n",
    "    ax.imshow(sample)\n",
    "\n",
    "    ax.set_title(f'{class_name}, {label}')\n",
    "    ax.axis('off')\n",
    "\n",
    "\n",
    "fig.suptitle('Random amples')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to Batchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(32 * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "baseline_model = BaselineModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=baseline_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, output):\n",
    "    y_index = torch.argmax(y, dim=1)\n",
    "    output_index = torch.argmax(output, dim=1)\n",
    "\n",
    "    return torch.mean((y_index == output_index).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, epochs=30, log=True):\n",
    "    train_loasses = []\n",
    "    trian_accuracies = []\n",
    "\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_loss = 0\n",
    "        train_acc = 0\n",
    "        val_acc = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for X_train, y_train in train_loader:\n",
    "            X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "            y_pred = model(X_train)\n",
    "\n",
    "            loss = criterion(y_pred, y_train)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_loss += loss.item() * y_train.shape[0]\n",
    "\n",
    "            train_acc += accuracy(y_train, y_pred)\n",
    "\n",
    "        train_loasses.append(batch_loss / len(train_loader.dataset))\n",
    "        trian_accuracies.append(train_acc / len(train_loader.dataset))\n",
    "\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "\n",
    "                y_pred =  model(X_val)\n",
    "\n",
    "                loss = criterion(y_pred, y_val)\n",
    "\n",
    "                batch_loss += loss.item() * y_val.shape[0]\n",
    "                val_acc += accuracy(y_val, y_pred)\n",
    "\n",
    "        validation_losses.append(batch_loss / len(val_loader.dataset))\n",
    "        validation_accuracies.append(val_acc / len(val_loader.dataset))\n",
    "\n",
    "        if log and (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Train loss: {np.round(train_loasses[-1], 3)}, Train acc: {np.round(trian_accuracies[-1], 3)}, Val loss: {np.round(validation_losses[-1], 3)}, Val acc: {np.round(validation_accuracies[-1], 3)}')\n",
    "\n",
    "    return train_loasses, trian_accuracies, validation_losses, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(trian_accuracies, validation_accuracies, epochs):\n",
    "    plt.figure(figsize=[10, 6])\n",
    "\n",
    "    plt.plot(range(1, epochs+1), trian_accuracies, c='blue', linestyle='--', marker='o')\n",
    "    plt.plot(range(1, epochs+1), validation_accuracies, c='red', linestyle='--', marker='o')\n",
    "\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss(train_loasses, validation_losses, epochs):\n",
    "    plt.figure(figsize=[10, 6])\n",
    "\n",
    "    plt.plot(range(1, epochs+1), train_loasses, c='blue', linestyle='--', marker='o')\n",
    "    plt.plot(range(1, epochs+1), validation_losses, c='red', linestyle='--', marker='o')\n",
    "\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_model(model, path):\n",
    "    torch.save(model, f=path)\n",
    "    print(f'model saved to {path} successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_loasses, trian_accuracies, validation_losses, validation_accuracies = train(\n",
    "    model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer, \n",
    "    criterion=criterion, \n",
    "    epochs=epochs, \n",
    "    log=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loasses, validation_losses, epochs=epochs)\n",
    "plot_acc(trian_accuracies, validation_accuracies, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(baseline_model, './baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Imporvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5) # 3 * 32 * 32 -> 64 * 27 * 27\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3) # 64 * 13 * 13 -> 64 * 11 * 11\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3) # 64 * 5 * 5 -> 63 * 3 * 3\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.averagepool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 64 * 3 * 3)\n",
    "        self.fc2 = nn.Linear(64 * 3 * 3, 64 * 3 * 3)\n",
    "        self.fc3 = nn.Linear(64 * 3 * 3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.averagepool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "impoved_model = BaselineModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=impoved_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_loasses, trian_accuracies, validation_losses, validation_accuracies = train(model=impoved_model, optimizer=optimizer, criterion=criterion, epochs=epochs, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loasses, validation_losses, epochs=epochs)\n",
    "plot_acc(trian_accuracies, validation_accuracies, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(impoved_model, './improvedmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5) # 3 * 32 * 32 -> 64 * 27 * 27\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3) # 64 * 13 * 13 -> 64 * 11 * 11\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3) # 64 * 5 * 5 -> 63 * 3 * 3\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.averagepool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 64 * 3 * 3)\n",
    "        self.fc2 = nn.Linear(64 * 3 * 3, 64 * 3 * 3)\n",
    "        self.fc3 = nn.Linear(64 * 3 * 3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.averagepool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "bn_model = BaselineModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=bn_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_loasses, trian_accuracies, validation_losses, validation_accuracies = train(model=bn_model, optimizer=optimizer, criterion=criterion, epochs=epochs, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loasses, validation_losses, epochs=epochs)\n",
    "plot_acc(trian_accuracies, validation_accuracies, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(bn_model, './bnmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5) # 3 * 32 * 32 -> 64 * 27 * 27\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3) # 64 * 13 * 13 -> 64 * 11 * 11\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3) # 64 * 5 * 5 -> 63 * 3 * 3\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.averagepool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 64 * 3 * 3)\n",
    "        self.fc2 = nn.Linear(64 * 3 * 3, 64 * 3 * 3)\n",
    "        self.fc3 = nn.Linear(64 * 3 * 3, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.averagepool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "do_model = BaselineModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(params=do_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_loasses, trian_accuracies, validation_losses, validation_accuracies = train(model=do_model, optimizer=optimizer, criterion=criterion, epochs=epochs, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loasses, validation_losses, epochs=epochs)\n",
    "plot_acc(trian_accuracies, validation_accuracies, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(bn_model, './domodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    baseline_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "            y_pred =  model(X_test)\n",
    "\n",
    "            loss = criterion(y_pred, y_test)\n",
    "\n",
    "            test_loss += loss.item() * y_test.shape[0]\n",
    "            test_accuracy += accuracy(y_test, y_pred)\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss, base_accuracy = test(model=baseline_model, test_loader=test_loader, criterion=criterion)\n",
    "print('baseline model:')\n",
    "print(f'loss= {np.round(base_loss, 3)}, accuracy= {np.round(base_accuracy, 3)}')\n",
    "\n",
    "impoved_loss, impoved_accuracy = test(model=impoved_model, test_loader=test_loader, criterion=criterion)\n",
    "print('impoved model:')\n",
    "print(f'loss= {np.round(impoved_loss, 3)}, accuracy= {np.round(impoved_accuracy, 3)}')\n",
    "\n",
    "bn_loss, bn_accuracy = test(model=bn_model, test_loader=test_loader, criterion=criterion)\n",
    "print('batch normalization model')\n",
    "print(f'loss= {np.round(bn_loss, 3)}, accuracy= {np.round(bn_accuracy, 3)}')\n",
    "\n",
    "do_loss, do_accuracy = test(model=do_model, test_loader=test_loader, criterion=criterion)\n",
    "print('drop out model')\n",
    "print(f'loss= {np.round(do_loss, 3)}, accuracy= {np.round(do_accuracy, 3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
