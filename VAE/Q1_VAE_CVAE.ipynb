{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vn26lahhFJd"
      },
      "source": [
        "# Variational AutoEncoders\n",
        "\n",
        "Full Name:\n",
        "\n",
        "Student ID:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJZ8AefthL95"
      },
      "source": [
        "\n",
        "# Variational Autoencoder\n",
        "\n",
        "In this notebook, you will implement a variational autoencoder and a conditional variational autoencoder with slightly different architectures and apply them to the popular MNIST handwritten dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuIiv2bhjFoC"
      },
      "source": [
        "Load several useful packages that are used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLdT7GSljI0f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "import torchvision.datasets as datasets\n",
        "import random\n",
        "import cv2\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "%matplotlib inline\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# for plotting\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "plt.rcParams['font.size'] = 16\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nqWhiLojS8M"
      },
      "source": [
        "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdQhVgi5jVQp"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('Good to go!')\n",
        "else:\n",
        "    print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcqRQILRjchz"
      },
      "source": [
        "## Load MNIST Dataset\n",
        "\n",
        "\n",
        "VAEs are notoriously finicky with hyperparameters, and also require many training epochs. In order to make this assignment approachable, we will be working on the MNIST dataset, which has 60,000 training and 10,000 test images. Each picture contains a centered image of white digit on black background (0 through 9). This was one of the first datasets used to train convolutional neural networks and it is fairly easy -- a standard CNN model can easily exceed 99% accuracy.\n",
        "\n",
        "To simplify our code here, we will use the PyTorch MNIST wrapper, which downloads and loads the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mExnwvTXjcF_"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "\n",
        "mnist_train = datasets.MNIST('./MNIST_data', train=True, download=True,\n",
        "                           transform=T.ToTensor())\n",
        "loader_train = DataLoader(mnist_train, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwDmYBjdhTrM"
      },
      "source": [
        "## Visualize dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2X_21cTwsox"
      },
      "source": [
        "It is always a good idea to look at examples from the dataset before working with it. Let's visualize the digits in the MNIST dataset. We have defined the function `show_images` to visualize the images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JMbbxMkwrYg"
      },
      "outputs": [],
      "source": [
        "def show_images(images):\n",
        "    images = torch.reshape(\n",
        "        images, [images.shape[0], -1]\n",
        "    )\n",
        "    sqrtn = int(math.ceil(math.sqrt(images.shape[0])))\n",
        "    sqrtimg = int(math.ceil(math.sqrt(images.shape[1])))\n",
        "\n",
        "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
        "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
        "    gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        ax = plt.subplot(gs[i])\n",
        "        plt.axis(\"off\")\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_aspect(\"equal\")\n",
        "        plt.imshow(img.reshape([sqrtimg, sqrtimg]))\n",
        "    return\n",
        "\n",
        "data_iter = iter(loader_train)\n",
        "imgs, _ = next(data_iter)\n",
        "imgs = imgs.view(batch_size, 784)\n",
        "show_images(imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOdQ3r5diEwr"
      },
      "source": [
        "# Fully Connected VAE\n",
        "\n",
        "Your first VAE implementation will consist solely of fully connected layers. You'll take the `1 x 28 x 28` shape of our input and flatten the features to create an input dimension size of 784. In this section you'll define the Encoder and Decoder models in the VAE class and implement the reparametrization trick, forward pass, and loss function to train your first VAE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOfC7oDrkUhl"
      },
      "source": [
        "## FC-VAE Forward\n",
        "\n",
        "Complete the VAE class by writing the initialization and the forward pass. The forward pass should pass the input image through the encoder to calculate the estimation of mu and logvar, reparametrize to estimate the latent space z, and finally pass z into the decoder to generate an image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple VAE class with MLP architecture\n",
        "\n",
        "class VAE_MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, device):\n",
        "        super(VAE_MLP, self).__init__()\n",
        "        # TODO: Define the architecture of the encoder and decoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            # TODO: Add layers for the encoder\n",
        "\n",
        "        )\n",
        "        self.fc_mu =\n",
        "        self.fc_logvar =\n",
        "        self.decoder = nn.Sequential(\n",
        "            # TODO: Add layers for the decoder\n",
        "\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # TODO: Implement the reparameterization trick\n",
        "\n",
        "        return z\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward pass\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "Wg7Yk0ODjWPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function\n",
        "\n",
        "Complete the vae_loss function below"
      ],
      "metadata": {
        "id": "qfNOoeJ3qPtR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF2ZUj2FjrFa"
      },
      "outputs": [],
      "source": [
        "# Define VAE loss function\n",
        "\n",
        "def vae_loss(recon, data, mu, logvar):\n",
        "    # TODO: Implement the reconstruction loss\n",
        "\n",
        "\n",
        "    # TODO: Implement the KL divergence loss\n",
        "\n",
        "\n",
        "    # TODO: Return the total loss as the sum of reconstruction and KL divergence losses\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV8fbzenkAXm"
      },
      "source": [
        "\n",
        "## Train a model\n",
        "\n",
        "Now that we have our VAE defined and loss function ready, lets train our model!\n",
        "\n",
        "Complete the train_vae_mlp function to train the network."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vae_mlp(model, train_loader, num_epochs=20, learning_rate=1e-3):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print()\n",
        "        print(45 * \"=\")\n",
        "        total_loss = []\n",
        "        for batch_idx, (data, _) in enumerate(train_loader):\n",
        "            # TODO: Forward process\n",
        "\n",
        "            # TODO: Flatten the data and recon tensors\n",
        "\n",
        "            # TODO: Calculate the loss using the vae_loss function\n",
        "\n",
        "            # TODO: Backpropagation and optimization step\n",
        "            loss =\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "        avg_loss = float(sum(total_loss)/len(total_loss))\n",
        "        print(f'VAE- Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}')"
      ],
      "metadata": {
        "id": "aOc6Z94M3AbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train VAE\n",
        "latent_size =\n",
        "VAE = VAE_MLP(input_dim= , hidden_dim= , latent_dim= , device=device)\n",
        "train_vae_mlp(VAE, loader_train)"
      ],
      "metadata": {
        "id": "Ph7_ua333SFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT6Ek-26jjJD"
      },
      "source": [
        "## Visualize results\n",
        "\n",
        "After training our VAE network, we're able to take advantage of its power to generate new training examples. This process simply involves the decoder: we intialize some random distribution for our latent spaces z, and generate new examples by passing these latent space into the decoder.\n",
        "\n",
        "Run the cell below to generate new images! You should be able to visually recognize many of the digits, although some may be a bit blurry or badly formed. Our next model will see improvement in these results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhhrsgrMTyTi"
      },
      "outputs": [],
      "source": [
        "num_samples = 10\n",
        "z = torch.randn(num_samples, latent_size).to(device=device)\n",
        "\n",
        "VAE.eval()\n",
        "samples = VAE.decoder(z).data.cpu().numpy()\n",
        "\n",
        "fig = plt.figure(figsize=(num_samples, 1))\n",
        "gspec = gridspec.GridSpec(1, num_samples)\n",
        "gspec.update(wspace=0.05, hspace=0.05)\n",
        "for i, sample in enumerate(samples):\n",
        "    ax = plt.subplot(gspec[i])\n",
        "    plt.axis('off')\n",
        "    if i == 4:\n",
        "      plt.title('Generated Samples of VAE')\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_aspect('equal')\n",
        "    plt.imshow(sample.reshape(28,28), cmap='Greys_r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzS_ufzEkhah"
      },
      "source": [
        "# Conditional FC-VAE\n",
        "\n",
        "The second model you'll develop will be very similar to the FC-VAE, but with a slight conditional twist to it. We'll use what we know about the labels of each MNIST image, and *condition* our latent space and image generation on the specific class. Instead of $q_{\\phi} (z|x)$ and $p_{\\phi}(x|z)$ we have $q_{\\phi} (z|x,c)$  and $p_{\\phi}(x|z, c)$\n",
        "\n",
        "This will allow us to do some powerful conditional generation at inference time. We can specifically choose to generate more 1s, 2s, 9s, etc. instead of simply generating new digits randomly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hle0JuhwklKc"
      },
      "source": [
        "## Define Network with class input\n",
        "\n",
        "Your CVAE architecture should be the same as your FC-VAE architecture, except you'll now concatenate a one-hot label vector to both the x input (in our case, the flattened image dimensions) and the z latent space.\n",
        "\n",
        "If the one-hot vector is called `c`, then `c[label] = 1` and `c = 0` elsewhere.\n",
        "\n",
        "For the `CVAE_MLP` class use the same FC-VAE architecture implemented in the last network.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple CVAE class with MLP architecture\n",
        "\n",
        "class CVAE_MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, num_classes, device):\n",
        "        super(CVAE_MLP, self).__init__()\n",
        "        # TODO: Define the architecture of the encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            # TODO: Add layers for the encoder\n",
        "\n",
        "        )\n",
        "        self.fc_mu =\n",
        "        self.fc_logvar =\n",
        "        self.fc_class =\n",
        "\n",
        "        # TODO: Define the architecture of the decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            # TODO: Add layers for the decoder\n",
        "\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # TODO: Implement the reparameterization trick\n",
        "\n",
        "        return\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y = y.view(y.size(0), -1)\n",
        "\n",
        "        # TODO: Concatenate x and y before passing them to the encoder\n",
        "\n",
        "        # TODO: Implement the forward pass\n",
        "\n",
        "\n",
        "        return\n"
      ],
      "metadata": {
        "id": "iGox0wzC-NWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to build helper functions, you can write the code in the below cell."
      ],
      "metadata": {
        "id": "FKw9Lidzwn55"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PHmq08N4w0Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function"
      ],
      "metadata": {
        "id": "IQMuHF9Yw3ZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the cvae_loss function below"
      ],
      "metadata": {
        "id": "yFIwbGnuw9qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CVAE loss function\n",
        "def cvae_loss(recon, data, mu, logvar, class_logits, labels):\n",
        "    # TODO: Flatten the data tensor\n",
        "\n",
        "    # TODO: Implement the reconstruction loss\n",
        "\n",
        "\n",
        "    # TODO: Implement the KL divergence loss\n",
        "\n",
        "\n",
        "    # TODO: Implement the cross-entropy loss for class prediction\n",
        "\n",
        "\n",
        "    # TODO: Return the total loss as the sum of reconstruction, KL divergence, and cross-entropy losses\n",
        "    return\n"
      ],
      "metadata": {
        "id": "5V181vOVxEHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUzKyFI9kp8i"
      },
      "source": [
        "## Train model\n",
        "\n",
        "Using the same training script, let's now train our CVAE!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1dzKDUsunbD"
      },
      "outputs": [],
      "source": [
        "# Training Loop - CVAE (MLP)\n",
        "def train_cvae_mlp(model, train_loader, num_epochs=10, learning_rate=1e-3):\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print()\n",
        "        print(45 * \"=\")\n",
        "        total_loss = []\n",
        "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "            # One-hot encode the labels\n",
        "\n",
        "            # TODO: Forward pass through the model and calculate the loss using cvae_loss\n",
        "\n",
        "\n",
        "            # TODO: Backpropagation and optimization step\n",
        "            loss =\n",
        "            total_loss.append(loss.item())\n",
        "\n",
        "\n",
        "        avg_loss = float(sum(total_loss)/len(total_loss))\n",
        "        print(f'CVAE-MLP Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train CVAE\n",
        "latent_size =\n",
        "CVAE = CVAE_MLP(input_dim= , hidden_dim= , latent_dim= , num_classes=10, device=device)\n",
        "train_cvae_mlp(CVAE, loader_train)"
      ],
      "metadata": {
        "id": "JgrqOKl4x0eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMAyFBZTkr1Y"
      },
      "source": [
        "## Visualize Results\n",
        "\n",
        "You've trained your CVAE, now let's conditionally generate some new data! This time, we can specify the class we want to generate by adding our one hot matrix of class labels. We use `torch.eye` to create an identity matrix, which effectively gives us one label for each digit. When you run the cell below, you should get one example per digit. Each digit should be reasonably distinguishable (it is ok to run this cell a few times to save your best results).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCfwpz0NALdZ"
      },
      "outputs": [],
      "source": [
        "num_samples = 10\n",
        "z = torch.randn(num_samples, latent_size)\n",
        "\n",
        "c = torch.eye(num_samples, 10)\n",
        "\n",
        "z = torch.cat((z,c), dim=-1).to(device='cuda')\n",
        "CVAE.eval()\n",
        "samples = CVAE.decoder(z).data.cpu().numpy()\n",
        "\n",
        "fig = plt.figure(figsize=(num_samples, 1))\n",
        "gspec = gridspec.GridSpec(1, num_samples)\n",
        "gspec.update(wspace=0.05, hspace=0.05)\n",
        "for i, sample in enumerate(samples):\n",
        "    ax = plt.subplot(gspec[i])\n",
        "    if i == 4:\n",
        "      plt.title('Generated Samples of CVAE')\n",
        "    plt.axis('off')\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    ax.set_aspect('equal')\n",
        "    plt.imshow(sample.reshape(28, 28), cmap='Greys_r')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}